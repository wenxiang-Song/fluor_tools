{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbac13c-bc28-4241-ad08-f64bb96d825e",
   "metadata": {},
   "source": [
    "# 目标分子的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd7be9-3c7f-4287-a3b1-a9a8409cd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标分子在 target.csv文件中，染料分子和溶剂分子分别位于smiles和sol两列，文件只有一行。\n",
    "# 最终目标是生成类似于的test的三个文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b24e7-8caa-461a-831b-62550309590b",
   "metadata": {},
   "source": [
    "### 1.1对应溶剂序号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f394c85-cab8-4e03-ac46-5cb9214d645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solvent_num 替换完成，结果已保存为：./input/input.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "data_file = './input/target.csv'  # 原始数据文件\n",
    "mapping_file = './data/00_solvent_mapping.csv'  # solvent 与 solvent_num 的映射表\n",
    "output_file = './input/input.csv'  # 替换后的输出文件\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(data_file)\n",
    "mapping_df = pd.read_csv(mapping_file)\n",
    "\n",
    "# 创建 solvent -> solvent_num 映射字典\n",
    "mapping_dict = dict(zip(mapping_df['solvent'], mapping_df['solvent_num']))\n",
    "\n",
    "# 替换原列中的 solvent_num\n",
    "df['solvent_num'] = df['solvent'].map(mapping_dict)\n",
    "\n",
    "# 保存为新的文件\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"solvent_num 替换完成，结果已保存为：{output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40e422-bc00-4d66-acf5-6db84ba6615d",
   "metadata": {},
   "source": [
    "### 1.2 生成分子性质数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3ca30ee-c88e-452d-b1c3-6e2ac5f07fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成分子性质预测\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdPartialCharges\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(\"./input/input.csv\")  # 替换为你的文件路径\n",
    "\n",
    "# 初始化存储计算结果的列表\n",
    "molecular_weights = []\n",
    "logP_values = []\n",
    "aromatic_ring_counts = []\n",
    "tpsa_values = []\n",
    "double_bond_counts = []\n",
    "ring_counts = []\n",
    "\n",
    "# 计算双键数量的函数\n",
    "def count_double_bonds(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None  # 处理无效的 SMILES\n",
    "    return sum(1 for bond in mol.GetBonds() if bond.GetBondType() == Chem.rdchem.BondType.DOUBLE or bond.GetIsAromatic())\n",
    "\n",
    "# 遍历 SMILES 计算各种性质\n",
    "for smiles in df['smiles']:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    if mol is not None:\n",
    "        # 计算分子量、logP、芳香环数量、TPSA、Gasteiger部分电荷\n",
    "        mw = Descriptors.MolWt(mol)  # 分子量\n",
    "        logP = Descriptors.MolLogP(mol)  # logP\n",
    "        num_aromatic_rings = Descriptors.NumAromaticRings(mol)  # 芳香环数量\n",
    "        tpsa = Descriptors.TPSA(mol)  # 近似极化率\n",
    "        \n",
    "        # 计算 Gasteiger 部分电荷\n",
    "        Chem.rdPartialCharges.ComputeGasteigerCharges(mol)\n",
    "        avg_charge = sum(atom.GetDoubleProp('_GasteigerCharge') for atom in mol.GetAtoms()) / mol.GetNumAtoms()\n",
    "\n",
    "        # 计算双键数量\n",
    "        double_bond_count = count_double_bonds(smiles)\n",
    "\n",
    "        # 获取环的信息并计算环的数量\n",
    "        rings = mol.GetRingInfo()\n",
    "        ring_count = rings.NumRings()\n",
    "    else:\n",
    "        mw = logP = num_aromatic_rings = tpsa = avg_charge = double_bond_count = ring_count = None  # 处理无效 SMILES\n",
    "\n",
    "    # 将计算结果添加到对应的列表\n",
    "    molecular_weights.append(mw)\n",
    "    logP_values.append(logP)\n",
    "    aromatic_ring_counts.append(num_aromatic_rings)\n",
    "    tpsa_values.append(tpsa)\n",
    "    double_bond_counts.append(double_bond_count)\n",
    "    ring_counts.append(ring_count)\n",
    "\n",
    "# 将计算结果添加到 DataFrame\n",
    "df['Molecular_Weight'] = molecular_weights\n",
    "df['LogP'] = logP_values\n",
    "df['TPSA'] = tpsa_values\n",
    "df['Double_Bond_Count'] = double_bond_counts\n",
    "df['Ring_Count'] = ring_counts\n",
    "\n",
    "# 保存到新的 CSV 文件\n",
    "df.to_csv(\"./input/input.csv\", index=False)\n",
    "print(\"完成分子性质预测\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a156ecf-5d50-48b9-8427-63a7ee3a28a7",
   "metadata": {},
   "source": [
    "### 1.3 对目标分子进行骨架定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e639eb4-4fe1-4a8e-99d9-c8bf9068f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：使用了示例骨架定义，请确保获取作者的完整16种骨架定义\n",
      "开始处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分配骨架标签: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 52.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到 ./input/input.csv\n",
      "\n",
      "骨架类型统计:\n",
      "5p6    1\n",
      "Name: tag_name, dtype: int64\n",
      "\n",
      "未分类分子比例: 0.00%\n",
      "处理完成!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 1. 导入作者的骨架定义\n",
    "try:\n",
    "    from FLAME.flsf.scaffold import scaffold  # 作者的原始骨架定义\n",
    "except ImportError:\n",
    "    # 如果无法导入，这里提供一个示例骨架定义（实际使用时替换为作者的完整定义）\n",
    "    scaffold = {\n",
    "    'SquaricAcid':[\n",
    "        'O=c1ccc1=O',\n",
    "        'O=C1CC([O-])C1',\n",
    "        'O=C1C=C(O)C1',\n",
    "        'OC1=CCC1',\n",
    "        'C=c1c(=O)c(=C)c1=O',\n",
    "        'c1ccc(N2CCC2)cc1',\n",
    "        'C=C1C(C=C1)=O',\n",
    "    ], \n",
    "    'Naphthalimide': [\n",
    "        'O=C1NC(=O)c2cccc3cccc1c23',\n",
    "        'O=C(C1=C2C(C=CC=C23)=CC=C1)NC3=O',\n",
    "    ], \n",
    "    'Coumarin': [\n",
    "        'C1=Cc2ccccc2OC1',\n",
    "        'O=c1ccc2ccccc2o1',\n",
    "        'S=c1ccc2ccccc2o1',\n",
    "        'O=C1C=Cc2ccccc2C1(F)F',\n",
    "        'O=c1ccc2ccccc2[nH]1',\n",
    "        'C[Si]1(C)C(=O)C=Cc2ccccc21',\n",
    "        'N=c1ccc2ccccc2o1',\n",
    "        'O=c1cnc2ccccc2o1',\n",
    "        'O=c1cnc2ccccc2[nH]1',\n",
    "    ], \n",
    "    'Carbazole': [\n",
    "        '[nH]1c2ccccc2c3ccccc13',\n",
    "    ], \n",
    "    'Cyanine':[\n",
    "        'NC=CC=O',\n",
    "        'NC=CC=[OH+]',\n",
    "        'NC=CC=[NH2+]',\n",
    "        'NC=CC=CC=O',\n",
    "        'NC=CC=CC=[OH+]',\n",
    "        'NC=CC=CC=[NH2+]',\n",
    "        'NC=CC=CC=CC=O',\n",
    "        'NC=CC=CC=CC=[OH+]',\n",
    "        'NC=CC=CC=CC=[NH2+]',\n",
    "        'NC=CC=CC=CC=CC=O',\n",
    "        'NC=CC=CC=CC=CC=[OH+]',\n",
    "        'NC=CC=CC=CC=CC=[NH2+]',\n",
    "        'NC=CC=CC=CC=CC=CC=O',\n",
    "        'NC=CC=CC=CC=CC=CC=[OH+]',\n",
    "        'NC=CC=CC=CC=CC=CC=[NH2+]',\n",
    "        'NC=CC=CC=CC=CC=CC=CC=O',\n",
    "        'NC=CC=CC=CC=CC=CC=CC=[OH+]',\n",
    "        'NC=CC=CC=CC=CC=CC=CC=[NH2+]',\n",
    "        'NC=CC=CC=CC=CC=CC=CC=CC=O',\n",
    "        'NC=CC=CC=CC=CC=CC=CC=CC=[OH+]',\n",
    "        'NC=CC=CC=CC=CC=CC=CC=CC=[NH2+]',\n",
    "    ],\n",
    "    # BODIPY\n",
    "    'BODIPY': [\n",
    "        'B(n1cccc1)n1cccc1',\n",
    "        'N1([BH2-]n2cccc2)C=CCC1',\n",
    "        '[BH2-](N1CC=CC1)n1cccc1',\n",
    "        'n1([BH2-][N+]2=CC=CC2)cccc1',\n",
    "        '[BH2-](n1cccc1)[N+]1=CC=CC1',\n",
    "        '[N+][BH2-][N+]',\n",
    "        'N[BH2-][N+]',\n",
    "        'N[BH2-]N',\n",
    "        '[N+]B[N+]',\n",
    "        'NB[N+]',\n",
    "        'NBN',\n",
    "    ], \n",
    "    'Triphenylamine': [\n",
    "        'c1ccc(cc1)N(c2ccccc2)c3ccccc3',\n",
    "        'C1=CC(=[N+](c2ccccc2)c2ccccc2)C=CC1',\n",
    "        'N=C1C=C/C(C=C1)=C(C2=CN=CS2)/C3=CN=CS3',\n",
    "    ], \n",
    "    'Porphyrin': [\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)[nH]5)C=C4)[nH]3',\n",
    "        'C1=Cc2cc3ccc(cc4cc(cc5ccc(cc1n2)[nH]5)C=N4)[nH]3',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)[nH]5)CC4)[nH]3',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)CC1=NC(=CC3=NC(=C2)C=C3)C=C1',\n",
    "        'C1=CC2=NC1=CC1=NC(=CC3=NC(=CC4=NC(=C2)C=C4)C=C3)C=C1',\n",
    "        'C1=C/C2=C/c3ccc([nH]3)CC3CCC(=N3)/C=C3/CC/C(=C/C1=N2)N3',\n",
    "        'C1=Cc2nc1ccc1ccc([nH]1)c1nc(ccc3ccc2[nH]3)C=C1',\n",
    "        'C1=CC2=NC1=Cc1ccc([n-]1)C=C1C=CC(=CC3=NC(=C2)C=C3)[NH2+]1',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5[nH]c(cc1n2)CC5)C=C4)[nH]3',\n",
    "        'c1cc2cc3nc(cc4ccc(cc5nc(cc1[nH]2)CC5)[nH]4)CC3',\n",
    "        'C1=C2C=c3ccc([nH]3)=Cc3ccc([n-]3)CC3=CC=C(CC(=C1)[NH2+]2)[NH2+]3',\n",
    "        'C1=C2C=c3ccc([nH]3)=Cc3ccc([n-]3)Cc3ccc([nH]3)CC(=C1)[NH2+]2',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5[nH]c(cc1n2)CC5)CC4)[nH]3',\n",
    "        'C1=CC2=NC1=CC1=NC(=CC3=NC(=CC3)C=c3ccc([n-]3)=C2)CC1',\n",
    "        'C1=Cc2nc1ccc1ccc([nH]1)c1nc(ccc3ccc2[nH]3)CC1',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)[nH]5)C=C4)s3',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)CC1C=CC(=N1)C=c1ccc([nH]1)=C2',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cn5ccc(cc1n2)c5)C=C4)[nH]3',\n",
    "        'C1=CC2=[NH+]C1=CC1=NC(C=C1)CC1C=CC(=N1)C=c1ccc([nH]1)=C2',\n",
    "        'C1=Cc2nc1cc1ccc([nH]1)c1ccc(cc3ccc(cc4ccc2[nH]4)o3)[nH]1',\n",
    "        'C=C1C=C2C=c3ccc([n-]3)=CC3=NC(=CC4=NC(=CC1=N2)C=C4)C=C3',\n",
    "        'C1=Cc2cc3ccc(cc4nc(c5ccc(ccc1n2)[nH]5)C=C4)[nH]3',\n",
    "        'C1=Cc2nc1ccc1nc(c3ccc(ccc4ccc2[n-]4)[n-]3)CC1',\n",
    "        'C=C1C=C2C=C3C=C4C(=O)CC(=C5CCC(=N5)C=c5ccc([n-]5)=CC1=N2)C4=N3',\n",
    "        'C=C1C=C2C=C3C=CC(=N3)C=C3C=CC(=N3)C=c3ccc([n-]3)=CC1=N2',\n",
    "        'C1=Cc2cc3[nH]c(cc4ccc(cc5nc(cc1n2)C=C5)[nH]4)CC3',\n",
    "        'C1=Cc2cc3ccc(cc4ccc(cc5cc(cc1n2)[NH+]=C5)[nH]4)[nH]3',\n",
    "        'C1=Cc2nc1ccc1nc(c3ccc(ccc4ccc2[n-]4)[nH]3)C=C1',\n",
    "        'C1=Cc2cc3cnc(cc4nc(cc5ccc(cc1n2)[n-]5)C=C4)[n-]3',\n",
    "        'C1=Cc2cc3ccc([nH]3)c3nc(ccc4ccc(cc1n2)[nH]4)C=C3',\n",
    "        'C=C1C=C2C=C3CCC(=CC4=NC(=CC5=NC(=CC1=N2)CC5)C=C4)N3',\n",
    "        'C1=CC2=NC1=Cc1ccc([n-]1)Cc1ccc([n-]1)C=C1C=CC(=N1)C2',\n",
    "        'C1=CC2=NC1=Cc1ccn(c1)CC1C=CC(=N1)C=c1ccc([nH]1)=C2',\n",
    "        'C1CC2CC3CCC(N3)C3CCC(CC4CCC(CC1N2)N4)N3',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)o5)C=C4)o3',\n",
    "        'c1c2nc(cc3ccc(cc4nc(cc5[nH]c1CC5)CC4)[nH]3)CC2',\n",
    "        'C=C1C=C2C=c3ccc([nH]3)=CC3=NC(=CC4=NC(=CC1=N2)CC4)C=C3',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)C=C1C=CC(=N1)CC1=NC(=C2)C=C1',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)C=C1C=CC(=N1)Cc1ccc([nH]1)C2',\n",
    "        'C1=CC2C=C3CCC(=N3)C=c3ccc([nH]3)=CC3=NC(=CC3)C=C1N2',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)C=C1C=CC(=N1)CC1C=CC(=C2)N1',\n",
    "        'C1=C2CCC(=N2)C=C2CCC(=N2)C=C2CCC(C=C3CCC1=N3)N2',\n",
    "        'c1c2nc(cc3ccc(cc4ccc(cc5nc1CC5)[n-]4)[n-]3)CC2',\n",
    "        'C1=Cc2nc1ccc1ccc([nH]1)c1ccc(ccc3nc2C=C3)[nH]1',\n",
    "        'C1=Cc2cc3ccc([n-]3)c3ccc(cc4nc(ccc1n2)C=C4)[n-]3',\n",
    "        'C1=CC2=NC1=CC1CCC(C=c3ccc([n-]3)=CC3=NC(=C2)CC3)[N-]1',\n",
    "        'C1=Cc2cc3ccc(cc4ccc(cc5nc(cc1n2)C=C5)[nH]4)[nH]3',\n",
    "        'O=C1CC2=CC3N=C(C=c4ccc([nH]4)=CC4=CCC(=N4)C=C1[N-]2)CC3=O',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)CC1=NC(=Cc3ccc([nH]3)C2)C=C1',\n",
    "        'O=C1C2=NC(=Cc3ccc([nH]3)C=C3C=CC(=Cc4ccc1[nH]4)[N]3)C=C2',\n",
    "        'C1=CC2=NC1=CC1=NC(=CC3=NC(=CC4=NC(=C2)C=C4)CC3)C=C1',\n",
    "        'C1=C2CCC(=Cc3ccc([nH]3)C=c3ccc([nH]3)=Cc3ccc1[nH]3)N2',\n",
    "        'C1=C2[CH]NC=1C=C1C=CC(=N1)C=C1C=CC(=N1)C=c1ccc([n-]1)=C2',\n",
    "        'C1=Cc2cc3ccc(cc4ccc(cc5nc(cc1n2)CC5)[nH]4)[nH]3',\n",
    "        'O=C1NC2=C=C1C=c1ccc([nH]1)=CC1=N[C](C=C1)C=c1ccc([n-]1)=C2',\n",
    "        'C1=Cc2cc3nc(cc4ccc([n-]4)c4ccc(ccc1n2)[n-]4)C=C3',\n",
    "        'C1=CC2=NC1=Cc1ccc([nH]1)C=C1C=CC(=N1)C=C1C=CC(=C2)[NH2+]1',\n",
    "        'C1=CC2=NC1=CC1=CCC(=N1)C=C1C=CC(=N1)C=c1ccc([n-]1)=C2',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)[nH]5)C=C4)o3',\n",
    "        'n([BH2-][N+]1=CCCC1=C2)(cc3c4cc(cc5)[nH]c5cc6nc(C=C6)c7)c2c3c(n4)cc8[nH]c7cc8',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)s5)C=C4)s3',\n",
    "        'C1=Cc2cc3ccc(cc4nc(cc5ccc(cc1n2)[n-]5)C=C4)[n-]3',\n",
    "        'C1=CC2=CC3=NC(=CC4=NC(=CC5=NC(=CC(=C1)[N-]2)C=C5)C=C4)C=C3',\n",
    "    ],\n",
    "    'PAHs': [\n",
    "        'c1ccc2ccccc2c1',\n",
    "        'c1ccc2cc3ccccc3cc2c1',\n",
    "        'c1ccc2c(c1)ccc3ccccc23',\n",
    "        'c1cc2ccc3cccc4ccc(c1)c2c34',\n",
    "        'c1cc2cccc3c4cccc5cccc(c(c1)c23)c45',\n",
    "        'c1ccc2c(c1)ccc3ccc4ccc5ccccc5c4c23',\n",
    "        'C1=CCC2C=c3ccccc3=CC2=C1', \n",
    "        'C12=CC=CC=C1C=C3C(C=C(C=CC=C4)C4=C3)=C2',\n",
    "        'C1(C(C=CC=C2)=C2C=C3)=C3C=CC=C1',\n",
    "        'C1(C(C=CC=C2)=C2C3=C4C=CC=C3)=C4C=CC=C1',\n",
    "        'C12=CC=C3B4N1C(C=CC4=CC=C3)=CC=C2',\n",
    "        'N12C=CC=CC1=CC=C3B2C=CC=C3',\n",
    "        'C=C1C=CC=C2NC=CC=C21',\n",
    "        'O=C1C2=C(C(C(C=C2)=O)=O)C=C3OC=CC=C31',\n",
    "        'O=C1C=C2NC=CC=C2C=C1',\n",
    "    ], \n",
    "    'Acridines': [\n",
    "        'B1c2ccccc2Cc2ccccc21',\n",
    "        'B1c2ccccc2Nc2ccccc21',\n",
    "        'C1=C2CCCC=C2Sc2ccccc21',\n",
    "        'C1=CC2=Cc3ccccc3CC2=CC1',\n",
    "        'C1=CC2=Cc3ccccc3[GeH2]C2=CC1',\n",
    "        'C1=CC2=Nc3ccccc3[SiH2]C2=CC1',\n",
    "        'C1=CC2=[O+]c3ccccc3CC2C=C1',\n",
    "        'C1=CC2C=c3cc4ccccc4[o+]c3=CC2NC1',\n",
    "        'C1=CC2C=c3ccccc3=[O+]C2C=C1',\n",
    "        'C1=CC2Oc3ccccc3CC2CC1',\n",
    "        'C1=CC=C[C-]2CC3=CC=C[C+]=C3OC=12',\n",
    "        'C1=CCC2=Cc3ccccc3[N]C2=C1',\n",
    "        'C1=CCC2Nc3ccccc3CC2C1',\n",
    "        'C1=CCC2Oc3ccccc3CC2C1',\n",
    "        'C1=C[C]2NC3C=CC=CC3C=C2C=C1',\n",
    "        'C1=Cc2[o+]c3ccccc3cc2CC1',\n",
    "        'C1=c2ccccc2=[SiH2+]c2ccccc21',\n",
    "        'C=c1ccc2c(c1)Oc1ccccc1C=2',\n",
    "        'C=c1ccc2c(c1)Sc1ccccc1C=2',\n",
    "        'N=C1C=CC2=Cc3ccccc3S(=O)(=O)C2=C1',\n",
    "        'N=C1C=CC2=Cc3ccccc3[SiH2]C2=C1',\n",
    "        'N=C1C=CC2Cc3ccccc3OC2=C1',\n",
    "        'N=C1c2ccccc2[GeH2]c2ccccc21',\n",
    "        'N=c1c2ccccc2oc2ccccc12',\n",
    "        'N=c1ccc2cc3ccccc3[nH]c-2c1',\n",
    "        'N=c1ccc2nc3ccccc3oc-2c1',\n",
    "        'O=C1C2=C(CC=CC2)C(=O)C2=C1CC=CC2',\n",
    "        'O=C1C2=C(CC=CC2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(CCC=C2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(CCCC2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(CCCC2)S(=O)(=O)c2ccccc21',\n",
    "        'O=C1C2=C(COC=C2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(COCC2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(OC=CC2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(OCC=C2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=C(OCCC2)C(=O)c2ccccc21',\n",
    "        'O=C1C2=CC=CCC2C(=O)c2ccccc21',\n",
    "        'O=C1C2=CC=CCC2Oc2ccccc21',\n",
    "        'O=C1C2=CCCCC2Oc2ccccc21',\n",
    "        'O=C1C2=CCCOC2C(=O)c2ccccc21',\n",
    "        'O=C1C2=CCOC=C2C(=O)c2ccccc21',\n",
    "        'O=C1C=C2C(=O)c3ccccc3CC2CC1',\n",
    "        'O=C1C=CC(=O)C2=C1CC1=C(O2)C(=O)C=CC1=O',\n",
    "        'O=C1C=CC(=O)c2c1[nH]c1ccccc1c2=O',\n",
    "        'O=C1C=CC(=O)c2c1oc1ccccc1c2=O',\n",
    "        'O=C1C=CC(=O)c2c1sc1ccccc1c2=O',\n",
    "        'O=C1C=CC2=Nc3ccccc3CC2=C1',\n",
    "        'O=C1C=CC2Cc3ccccc3OC2=C1',\n",
    "        'O=C1C=CC2Cc3ccccc3OC2C1',\n",
    "        'O=C1C=CC=C2C(=O)c3ccccc3C=C12',\n",
    "        'O=C1C=Cc2c([nH]c3ccccc3c2=O)C1',\n",
    "        'O=C1c2ccccc2C(=O)C2C=CC=CC12',\n",
    "        'O=C1c2ccccc2OC2C=CC=CC12',\n",
    "        'O=[Te+]1=c2ccccc2=Cc2ccccc21',\n",
    "        'O=c1c2c(oc3ccccc13)C=CCC2',\n",
    "        'O=c1c2c(sc3ccccc13)CCC=C2',\n",
    "        'O=c1c2ccccc2[se]c2ccccc12',\n",
    "        'O=c1c2ccccc2oc2ccccc12',\n",
    "        'O=c1c2ccccc2sc2ccccc12',\n",
    "        'O=c1cc2oc3ccccc3nc-2c2ccccc12',\n",
    "        'O=c1cc2oc3ccccc3nc-2c2cccnc12',\n",
    "        'O=c1ccc(=O)c2c(=O)c3ccccc3c(=O)c1=2',\n",
    "        'O=c1ccc2cc3ccccc3[nH]c-2c1',\n",
    "        'O=c1ccc2cc3ccccc3oc-2c1',\n",
    "        'O=c1cccc2[nH]c3ccccc3cc1-2',\n",
    "        'S=c1c2ccccc2[se]c2ccccc12',\n",
    "        'S=c1c2ccccc2oc2ccccc12',\n",
    "        'S=c1c2ccccc2sc2ccccc12',\n",
    "        '[CH]1C2=CC=CCC2=[NH+]c2ccccc21',\n",
    "        '[CH]1C=C2Nc3ccccc3CC2=C[CH+]1',\n",
    "        '[N+]=C1C=CC2=Cc3ccccc3[BH2-]C2=C1',\n",
    "        '[O+]=c1cccc2sc3ccccc3cc1-2',\n",
    "        '[OH+]=c1cccc2oc3ccccc3cc1-2',\n",
    "        'c1ccc2[o+]c3ccccc3cc2c1',\n",
    "        'c1ccc2[o+]c3ccccc3nc2c1',\n",
    "        'c1ccc2[s+]c3ccccc3cc2c1',\n",
    "        'c1ccc2[s+]c3ccccc3nc2c1',\n",
    "        'c1ccc2[se+]c3ccccc3cc2c1',\n",
    "        'c1ccc2[te+]c3ccccc3cc2c1',\n",
    "        'c1ccc2c(c1)Cc1ccccc1C2',\n",
    "        'c1ccc2c(c1)Cc1ccccc1N2',\n",
    "        'c1ccc2c(c1)Cc1ccccc1O2',\n",
    "        'c1ccc2c(c1)Cc1ccccc1S2',\n",
    "        'c1ccc2c(c1)Cc1ccccc1[Se]2',\n",
    "        'c1ccc2c(c1)Cc1ccccc1[SiH2]2',\n",
    "        'c1ccc2c(c1)Nc1ccccc1N2',\n",
    "        'c1ccc2c(c1)Nc1ccccc1O2',\n",
    "        'c1ccc2c(c1)Nc1ccccc1S2',\n",
    "        'c1ccc2c(c1)Oc1ccccc1O2',\n",
    "        'c1ccc2c(c1)Oc1ccccc1S2',\n",
    "        'c1ccc2c(c1)Sc1ccccc1S2',\n",
    "        'c1ccc2c(c1)[SiH2]c1ccccc1[SiH2]2',\n",
    "        'c1ccc2nc3ccccc3cc2c1',\n",
    "        'c1ccc2nc3ccccc3nc2c1',\n",
    "        'c1ccc2pc3ccccc3cc2c1',\n",
    "        'O=C1C=Cc2c(cc3occcc-3c2=O)C1=O',\n",
    "    ], \n",
    "    # 6+5\n",
    "    '5p6':[\n",
    "        'c1ccc2[nH]ccc2c1',\n",
    "        'c1ccc2occc2c1',\n",
    "        'c1ccc2sccc2c1',\n",
    "        'c1ccc2[nH]cnc2c1',\n",
    "        'c1ccc2scnc2c1',\n",
    "        'c1ccc2ocnc2c1',\n",
    "        'c1ccc2nonc2c1',\n",
    "        'c1ccc2nsnc2c1',\n",
    "        'c1ccc2scpc2c1',\n",
    "        'C1=Nc2ccccc2C1',\n",
    "        'c1ccn2cccc2c1',\n",
    "        'c1ccn2ccnc2c1',\n",
    "        'c1ccc2[nH]ncc2c1',\n",
    "        'c1ccn2cnnc2c1',\n",
    "        'c1ccc2cscc2c1',\n",
    "        'c1ncc2nc[nH]c2n1',\n",
    "        'C1=COc2n[nH]cc2C1',\n",
    "        'c1cnc2ncnn2c1',\n",
    "        'c1ccc2c(c1)CCO2',\n",
    "        'c1cn2ccnc2cn1',\n",
    "        'c1cnc2sccc2c1',\n",
    "        'O=C1OCc2ccccc21',\n",
    "        'c1cc[n+]2c(c1)[N-][NH2+]C2',\n",
    "        'O=C1NCc2nc[nH]c2N1',\n",
    "        'c1ccc2c(c1)OCO2',\n",
    "        'O=C1Cc2ccccc2C1=O',\n",
    "        'c1ccc2[nH]nnc2c1',\n",
    "        'O=C1N=Cc2ccccc21',\n",
    "        'c1ccc2c(c1)=NCN=2',\n",
    "        'c1cnn2cccc2c1',\n",
    "        'c1ccc2n[se]nc2c1',\n",
    "        'C1=CC2=CCCN2N=C1',\n",
    "        'c1nc2cnc[nH]c-2n1',\n",
    "        'C=[N+]1[BH2-]N2C=CC=CC2=N1',\n",
    "        'N=C1N=CC2N=CNC2N1',\n",
    "        'O=c1ccc2sccc2[nH]1',\n",
    "        'C1=CC2CCCC2CC1',\n",
    "        'O=C1NC(=O)C2CC=CCC12',\n",
    "        'O=C1CCCc2occc21',\n",
    "        'c1scc2c1OCCO2',\n",
    "        'O=c1ccn2nccc2o1',\n",
    "        'c1ccn2cncc2c1',\n",
    "        'O=C1NC(=O)C2CCCCC12',\n",
    "        'c1cnc2[nH]cnc2c1',\n",
    "        'c1cc2nc[nH]c2cn1',\n",
    "        'c1ccc2cocc2c1',\n",
    "        'c1ncc2ccsc2n1',\n",
    "        'c1cc2sccc2cn1',\n",
    "        'c1ncc2cc[nH]c2n1',\n",
    "        'N=c1ncc2[nH]ccc2[nH]1',\n",
    "        'c1cnc2[nH]cnc2n1',\n",
    "        'O=c1ccsc2ncnn12',\n",
    "        'c1cnc2nccn2c1',\n",
    "        'c1cc2cnccn2c1',\n",
    "        'C1=Cc2ccccc2C1',\n",
    "        'O=S1(=O)NC=Cc2sccc21',\n",
    "        'c1ccc2[pH]ccc2c1',\n",
    "        'C=C1Nc2ccccc2O1',\n",
    "        'c1cnc2[nH]ncc2c1',\n",
    "        'c1ncc2c[nH]nc2n1',\n",
    "        'c1ncn2c1CCCC2',\n",
    "        'c1cc2[o+]ccc-2c[nH]1',\n",
    "        'O=C1NCc2ccccc21',\n",
    "        'O=[PH]1C=Cc2ccccc21',\n",
    "        'c1ncc2sccc2n1',\n",
    "        'c1cc2ccsc2cn1',\n",
    "        'O=c1nc[nH]n2cncc12',\n",
    "        'c1cnc2ncsc2c1',\n",
    "        'c1nncc2oncc12',\n",
    "        'O=c1ccc2c[nH]ccn1-2',\n",
    "        'N=c1ncnc2[nH][nH]cc1-2',\n",
    "        'C=C1Nc2ccccc2S1',\n",
    "        'C1=Nn2cnnc2SC1',\n",
    "        'c1ncn2cncc2n1',\n",
    "        'c1cc2c[nH]nc2cn1',\n",
    "        'O=c1[nH]ccn2nccc12',\n",
    "        'O=c1cnn2cnnc2[nH]1',\n",
    "        'O=c1[nH]ncn2cnnc12',\n",
    "        'c1ccc2oncc2c1',\n",
    "        'c1cc2cc[nH]c2cn1',\n",
    "        'c1ncc2cscc2n1',\n",
    "        'c1cnn2cnnc2c1',\n",
    "        'O=c1ccn2cnnc2s1',\n",
    "        'c1ccn2nccc2c1',\n",
    "        'N=C1CSc2nncn2N1',\n",
    "        'O=c1ccnc2sccn12',\n",
    "        'O=c1cnc2c[nH]ccn1-2',\n",
    "        'C=c1sc2n(c1=O)CC=CN=2',\n",
    "        'c1ccc2c(c1)N=S=N2',\n",
    "        'O=C1C=Nc2cncc(=O)n21',\n",
    "        'c1cnc2cscc2n1',\n",
    "        'O=c1[nH]ncc2nn[nH]c12',\n",
    "        'O=C1CN=C2C=NC=CN12',\n",
    "        'O=C1C=NN2CNN=C2N1',\n",
    "        '[CH]1C=CC=C2C=CC=C12',\n",
    "        'c1cc2[s+]ccc-2c[nH]1',\n",
    "        'O=c1ccnc2n1CCS2',\n",
    "        'C=C1N=Cc2ccccc21',\n",
    "        'C=C1Nc2ccccc2N1',\n",
    "        'C=C1C(=O)c2ccccc2C1=O',\n",
    "        'N=c1[nH]ncc2nn[nH]c12',\n",
    "        'c1cnc2ncoc2c1',\n",
    "        'C=C1Sc2ccccc2C1=O',\n",
    "        'C1=CCC2CCCC2=C1',\n",
    "        'c1ccc2c(c1)CCN2',\n",
    "        'O=S1(=O)C=Cc2ccccc21',\n",
    "        'B1Oc2ccccc2O1',\n",
    "        'c1cc2nonc2cn1',\n",
    "        'O=c1[nH]nnc2ccnn12',\n",
    "        '[BH2-]1[O+]=CN=C2SC=NN12',\n",
    "        'C=C1N=C2SC=NN2[BH2-]O1',\n",
    "        'C1=CC2=CCCC2CC1',\n",
    "        'C1=CC2=NNCC2CC1',\n",
    "        'C=C1C(=C)c2ccccc2C1=C',\n",
    "        'c1ccc2[se]c[nH+]c2c1',\n",
    "        'C=C1Nc2ccccc2[Se]1',\n",
    "        'C=[N+]1[BH2-][n+]2ccccc2[N-]1',\n",
    "        'c1cnc2[nH]ccc2c1',\n",
    "        'c1ccc2c[nH]cc2c1',\n",
    "        'N=C1N=Cc2ccccc21',\n",
    "        'O=c1ccnc2[nH][nH]c[n+]1-2',\n",
    "        'C1=Cc2ccccc2[SiH2]1',\n",
    "        'C=C1N=CC2=C1CCCC2',\n",
    "        'C=C1CC2=CC(=S)C=CC2=[NH+]1',\n",
    "        'N=C1N=C2C=CC=CN2C1=N',\n",
    "        'C=C1N=C2SC=CN2[BH2-]O1',\n",
    "        'C=C1N=c2ccccc2=[O+]1',\n",
    "        'N=c1[nH][nH]c2nccc(=O)n12',\n",
    "        'O=C1C=NC2=CN=CCN12',\n",
    "        'c1[nH]cc2c1CCCC2',\n",
    "        'C=C1CCCC2CCCC12',\n",
    "        'C=C1C=CCC2COCC12',\n",
    "        'C1=CC2CCCN2N=C1',\n",
    "    ],\n",
    "    # 6+6\n",
    "    '6p6': [\n",
    "        'c1ccc2c(c1)CCCN2',\n",
    "        'C1=CB2C(=CC=C3C=CC=CN23)C=C1',\n",
    "        'c1ccc2ncccc2c1',\n",
    "        'C1=CNC2=NCN=CC2=N1',\n",
    "        'c1ccc2c(c1)CCCO2',\n",
    "        'N=c1n[nH+]c2ccccc2[nH]1',\n",
    "        'O=C1C=CC(=O)c2ccccc21',\n",
    "        'c1cnc2c(c1)CCCC2',\n",
    "        'C1=COC2=C(C1)CCCC2',\n",
    "        'O=c1ccoc2ccccc12',\n",
    "        'c1ccc2cnccc2c1',\n",
    "        'c1ccc2ncncc2c1',\n",
    "        'O=C1CCCC2=C1CC=CN2',\n",
    "        'C1=NCNc2ccccc21',\n",
    "        'c1cnc2ncccc2c1',\n",
    "        'O=c1occc2ccccc12',\n",
    "        'C1=Cc2ccccc2SC1',\n",
    "        'C=C1C=c2ccccc2=[O+]C1=O',\n",
    "        'O=c1cnc2cncnc2[nH]1',\n",
    "        'c1cc2c(cn1)CCCC2',\n",
    "        'O=c1ccnc2ccccn12',\n",
    "        'C1=CC2CCCCC2CC1',\n",
    "        'C=C1C=C2CCCNC2=CC1=[OH+]',\n",
    "        'c1cc[n+]2ccccc2c1',\n",
    "        'c1cc2nncnc2cn1',\n",
    "        'C=C1NCCc2ccccc21',\n",
    "        'O=C1NCNc2ccccc21',\n",
    "        'O=c1ccnc2cnccn12',\n",
    "        'O=C1C=CC2=CNCCC2=C1',\n",
    "        'O=C1C=C2CCCCC2CC1',\n",
    "        'O=c1ccc2cccoc-2c1',\n",
    "        'O=C1C=Cc2ncccc2C1',\n",
    "        'O=S1(=O)NC=Cc2ccccc21',\n",
    "        'C=c1ccc2c(c1)C=CC(=[NH2+])C=2',\n",
    "        'O=c1nc2ccccn2c(=O)[nH]1',\n",
    "        'C=c1ccc2c(c1)C=CC(=O)C=2',\n",
    "        'N=c1ncc2nccnc2[nH]1',\n",
    "        '[BH2-]1OC=Cc2cccc[n+]21',\n",
    "        'O=C1CCC2CCCCC2C1',\n",
    "        'C=C1C=CC(=O)c2ncccc21',\n",
    "        'c1ccc2nccnc2c1',\n",
    "        'S=c1cc[n+]2ccccc2[nH]1',\n",
    "        'C1=NNc2ccccc2S1',\n",
    "        'C1=CSC2=CCCCC2=C1',\n",
    "        'c1ccc2[o+]cccc2c1',\n",
    "        'O=C1C=NNC2=NN=CNN12',\n",
    "        'O=C1CCC2COC=CC2C1',\n",
    "        'O=C1C=C2C=COCC2CC1',\n",
    "        'N=c1nc2ncccc2c[nH]1',\n",
    "        'C=C1C=Cc2cccnc2C1=O',\n",
    "        'C1=Cc2ccccc2CC1',\n",
    "        'O=c1cnc2cnccc2[nH]1',\n",
    "        'c1cc2c(nn1)CCCC2',\n",
    "        'O=c1cnc2cccnc2[nH]1',\n",
    "        'N=c1ccc2c[nH]ccc-2c1',\n",
    "        'C=C1C=CNc2ccccc21',\n",
    "        'O=c1ccc2c(=O)occc2o1',\n",
    "        '[BH2-]1NC=Cc2cccc[n+]21',\n",
    "        'N=C1C=CC2=CNCCC2=C1',\n",
    "        'O=c1[nH]c(=O)c2nccnc2[nH]1',\n",
    "        'O=c1[nH]ncc2ccccc12',\n",
    "        'N=c1[nH]ccc2ncccc12',\n",
    "        'C1OCC2OCOCC2O1',\n",
    "        'O=C1C=C2C=CCCC2CO1',\n",
    "        'C1=COc2ccccc2C1',\n",
    "        'c1ncc2c(n1)CCCC2',\n",
    "        '[BH2-]1OCC=C2C=CN=CN12',\n",
    "        'C=C1NS(=O)(=O)c2ccccc2C1=O',\n",
    "        'O=c1[nH]c(=O)c2ncnnc2[nH]1',\n",
    "        'C=C1C(=O)C=Cc2ccc(=O)oc21',\n",
    "        'N=c1ncc2c([nH]1)NNC=N2',\n",
    "        'O=c1nc[nH]c2ncccc12',\n",
    "        'C1=COC2=CCCCC2=C1',\n",
    "        'c1ccc2c(c1)OCCO2',\n",
    "        '[BH2-]1[O+]=CC=C2C=CN=CN12',\n",
    "    ],\n",
    "    # 6+n+5\n",
    "    '5n6':[\n",
    "        'c1ccc(CC2=NCCC2)cc1',\n",
    "        'C=C1CCCC1=CC1CCCCC1',\n",
    "        'c1ccc(Cc2cnsc2)cc1',\n",
    "        'O=C1CCCC1=CC1CCCCC1',\n",
    "        'c1ccc(CC2=NCNC2)cc1',\n",
    "        'c1ccc(-c2ccno2)cc1',\n",
    "        'c1ccc(Cc2nnco2)cc1',\n",
    "        'c1ccc(-c2ccco2)cc1',\n",
    "        'O=C1N=CC=CC1C1CCCO1',\n",
    "        'c1ccc(Cc2ncco2)cc1',\n",
    "        'O=C1CCCC1=Cc1ccccc1',\n",
    "        'c1ccc(Cc2cccs2)cc1',\n",
    "        'c1ccc(-c2cnco2)cc1',\n",
    "        'c1cc(-c2nccs2)ccn1',\n",
    "        'c1ccc(CC2=NCCN2)cc1',\n",
    "        'c1cc(N2CCCC2)ccn1',\n",
    "        'c1ccc(Cc2ccon2)cc1',\n",
    "        'c1ccc(Cc2cncs2)cc1',\n",
    "        'c1ccc(C2=NCCO2)cc1',\n",
    "        'c1ccc(-c2ncco2)cc1',\n",
    "        'C1=C(Cc2ccoc2)CCCC1',\n",
    "        'O=C1N=CC=C1c1ccccc1',\n",
    "        'O=C1NCC=C1Cc1ccccc1',\n",
    "        'c1ccc(Cc2nccs2)cc1',\n",
    "        'c1ccc(-n2cccn2)nc1',\n",
    "        'c1ccc(Cc2ccco2)cc1',\n",
    "        'O=c1ncccn1C1=CCCO1',\n",
    "        'S=c1sscc1Cc1ccccc1',\n",
    "        'c1ccc(-c2cccs2)cc1',\n",
    "        'C(=Nc1cccs1)c1cccs1',\n",
    "        'c1ccc(-n2cccc2)cc1',\n",
    "        'C(=Nc1ccccc1)c1cncs1',\n",
    "        'C=C1N=C(Cc2ccccc2)OC1=O',\n",
    "        'O=C1C=CC(CC2=CCCC2)=CC1',\n",
    "        'C(=NN=Cc1ccco1)c1ccccc1',\n",
    "        'C1=CC(=Cc2nccs2)C=CO1',\n",
    "        'S=C1C=CC(C2CCCO2)C=N1',\n",
    "        'c1ccc(Cc2ccc[nH]2)cc1',\n",
    "        'c1ccc(Cc2c[nH]cn2)cc1',\n",
    "        'C(=Cc1ccco1)c1ccccc1',\n",
    "        'C1=CC(=Cc2ccc[nH]2)C=CO1',\n",
    "        'C(=Cc1cccs1)c1ccccc1',\n",
    "        'O=C1N=CC(=Cc2ccccc2)S1',\n",
    "        'O=C1C=CC(=Cc2ccccn2)N1',\n",
    "        'c1ccc(-c2ncc[nH]2)cc1',\n",
    "        'C=C1C=C(c2ccccc2)C(=O)O1',\n",
    "        'O=C1CCCC1=CC=Cc1ccccc1',\n",
    "        'C1=CC(=Cc2ccccn2)N=C1',\n",
    "        'c1ccc(Cc2cc[nH]c2)cc1',\n",
    "        'C(=Cc1ccccc1)C1=CCOC1',\n",
    "        'C=C1C=NC(Cc2ccccc2)=C1',\n",
    "        'O=c1nc(-n2cncn2)cc[nH]1',\n",
    "        'O=C1NC=NC1=Cc1ccncc1',\n",
    "        'C1=CC(=CC2C=CCCC2)N=C1',\n",
    "        'O=c1scc(Cc2ccccc2)s1',\n",
    "        'C1=CC(=Cc2cncs2)C=CO1',\n",
    "        'O=C1NC(=S)SC1=Cc1ccccc1',\n",
    "        'C=C1N=CN(c2ccccc2)C1=O',\n",
    "        'c1ccc(Cc2ccn[nH]2)cc1',\n",
    "        'C(#Cc1cccs1)c1ccccc1',\n",
    "        'c1cc[n+](C2CCCO2)cc1',\n",
    "        'c1ccc(-c2ccc[nH]2)cc1',\n",
    "        'O=C1C=CC=C1Cc1ccccc1',\n",
    "        'O=C1NC=NC1=Cc1ccccc1',\n",
    "        'O=C1C=CC=CC1=C1C=S=CS1',\n",
    "        'C=C1C=C(c2ccccc2)C=N1',\n",
    "        'c1ccc(Cc2nnc[nH]2)cc1',\n",
    "        'C1=CC(=Cc2ccco2)C=CO1',\n",
    "        'C1=CC(=Cc2cccs2)C=CO1',\n",
    "        'O=C(Nc1ccccc1)c1cncs1',\n",
    "        'c1ccc(-c2cccs2)nc1',\n",
    "        'C=C1N=CN(CCN=Cc2ccccc2)C1=O',\n",
    "        'C=C1CCN(C(=O)CN=Cc2ccccc2)C1',\n",
    "        'c1cc(CNC2CCCCC2)cs1',\n",
    "        'c1csc(C2CCCCC2)c1',\n",
    "        'C1=C(c2cccs2)CCCC1',\n",
    "        'C1=CC(C=NN=Cc2ccccc2)=S=C1',\n",
    "        'c1ccc(-n2cccn2)cc1',\n",
    "        'c1ccc(-n2cnnn2)cc1',\n",
    "        'c1ccc(-c2nncs2)cc1',\n",
    "        'C1=NC(c2ccccc2)=NC1',\n",
    "        'O=C1C=C(CC2=CC(=O)OC2)CCC1',\n",
    "        'c1csc(CNC2CCCCC2)c1',\n",
    "        'O=C1CC=C(c2ccccc2)N1',\n",
    "        'c1ccc(CCc2cccs2)cc1',\n",
    "        'c1csc(N2CCCCC2)c1',\n",
    "        'c1ccc(-c2ccsc2)cc1',\n",
    "        'c1csc(-c2cnnnn2)c1',\n",
    "        'C=C1OC(=O)C(=Cc2ccccc2)C1=O',\n",
    "        'O=C(c1ccccc1)c1cnoc1',\n",
    "        'c1ccc(-c2cn[nH]n2)cc1',\n",
    "        'C1=C(Cc2ccccc2)COC1',\n",
    "        'c1cc(-n2nccn2)ccn1',\n",
    "        'c1ccc(-c2cncs2)cc1',\n",
    "        'c1ccc(-c2cscn2)cc1',\n",
    "        'c1ccc(-n2nccn2)cc1',\n",
    "        'C1=CC(Cc2ccccc2)=S=C1',\n",
    "        'O=C(C=Cc1ccccc1)c1cc[cH-]c1',\n",
    "        'C1=C(Cc2ccccc2)CCC1',\n",
    "        'C(=Cc1ccc[nH]1)c1ccccc1',\n",
    "        'O=C1CCCC=C1Cc1cccs1',\n",
    "        'C1=CC(=Cc2ccccc2)N=C1',\n",
    "        'C=C1CCCC1=Cc1ccccc1',\n",
    "        'c1ccc(-c2nnco2)cc1',\n",
    "        'C1=NC(c2ccccc2)=CC1',\n",
    "        'C(CSc1nnc[nH]1)=NN=Cc1ccccc1',\n",
    "        'O=C1CCCC=C1Cc1ccco1',\n",
    "        'C1=NC(c2ccccc2)CO1',\n",
    "        'O=c1c[nH+]n(Cc2ccccc2)o1',\n",
    "        'C(=Cc1ccccc1)SC=C1CCCC1',\n",
    "        'c1ccc(-c2ncc[se]2)cc1',\n",
    "        'C1=CNC(=CC=C2C=CCS2)C=C1',\n",
    "        'O=C1NCCC1=Cc1ccccc1',\n",
    "        'C=C(C=C1C=CC=C1)c1ccccc1',\n",
    "        'O=C(OC1=NCC=C1)c1ccccc1',\n",
    "        'N=c1ncn(C2CCCO2)cn1',\n",
    "        'c1csc(-c2ncncn2)c1',\n",
    "        'C(=Cc1cnco1)c1ccccc1',\n",
    "        'c1ccc(Cc2ncon2)cc1',\n",
    "        'c1ccc(C2=NCCN2)cc1',\n",
    "        'C(=Cc1ncco1)c1ccccc1',\n",
    "        'c1ccc(CCc2ccc[nH]2)cc1',\n",
    "        'O=C1CNC=C1Cc1ccccc1',\n",
    "        'C1=NCC(c2ccccc2)O1',\n",
    "        'c1cc(-c2ncco2)ccn1',\n",
    "        'c1ccc(N=c2nc[nH]s2)cc1',\n",
    "        'c1ccc(-c2c[nH]cn2)cc1',\n",
    "        'c1ccc(-c2ccn[nH]2)cc1',\n",
    "        'C=C1C=C(OC(=O)c2ccccc2)C(=O)O1',\n",
    "        'c1ccc(Cc2cn[nH]c2)cc1',\n",
    "        'C1=CNB(c2ccccc2)N1',\n",
    "        'C1=CSC(=C2C=CNC=C2)[N-]1',\n",
    "    ],\n",
    "    # 6+n+6\n",
    "    '6n6': [\n",
    "        'c1ccc(Cc2ccccc2)cc1',\n",
    "        '[N+]=C1C=CC(=NN=C2C=CC(=[N+])C=C2)C=C1',\n",
    "        'c1ccc(CCc2ccccc2)cc1',\n",
    "        'c1ccc(CNc2ccccc2)cc1',\n",
    "        'C(=Cc1ccccc1)Cc1ccccc1',\n",
    "        'c1ccc(Nc2ccccc2)cc1',\n",
    "        'c1ccc(Oc2ccccc2)cc1',\n",
    "        'B(c1ccccc1)c1ccccc1',\n",
    "        'c1ccc(Pc2ccccc2)cc1',\n",
    "        'C(=Cc1ccccc1)c1ccccc1',\n",
    "        'C(#Cc1ccccc1)c1ccccc1',\n",
    "        'C(C#Cc1ccccc1)#Cc1ccccc1',\n",
    "        'C(C=Cc1ccccc1)=Cc1ccccc1',\n",
    "        'C(=Cc1ccccc1)CCCC=Cc1ccccc1',\n",
    "        'C(=CC=Cc1ccccc1)C=Cc1ccccc1',\n",
    "        'C(=Cc1ccccc1)CCCc1ccccc1',\n",
    "        'C(#Cc1ccccc1)C=Cc1ccccc1',\n",
    "        'c1ccc(-c2ccccc2)cc1',\n",
    "        'c1ccc(COc2ccccc2)cc1',\n",
    "        'c1ccc(-c2ccncn2)cc1',\n",
    "        'c1ccc(Cc2ncccn2)cc1',\n",
    "        'C(=NNCc1ccccc1)c1ccccc1',\n",
    "        'C1=CNCC(Cc2ccccc2)=C1',\n",
    "        'C1=COC(c2ccccc2)=CC1',\n",
    "        'C1=COC(OC2CCCCO2)CC1',\n",
    "        'C1=CCN(Cc2ccccc2)C=C1',\n",
    "        'C(C=Cc1ccccc1)=CCC=Cc1ccccc1',\n",
    "        'C1=CCCC(Cc2ccccc2)=C1',\n",
    "        'c1ccc(Cc2ccccn2)cc1',\n",
    "        'C1=CCC(OCc2ccccc2)=CC1',\n",
    "        'C1=CNCC(Cc2ccccc2)=N1',\n",
    "        'C1=CCOC(c2ccccc2)=C1',\n",
    "        'C1=COC(C=Cc2ccccc2)=CC1',\n",
    "        'C(=Cc1cccnc1)c1ccccc1',\n",
    "        'c1ccc(-c2cnccn2)cc1',\n",
    "        'c1ccc(Cc2cnccn2)cc1',\n",
    "        'c1ccc(-c2ccccn2)nc1',\n",
    "        'C1=C(Cc2ccccc2)COCC1',\n",
    "        'C1=C(Cc2ccccc2)CNCN1',\n",
    "        'c1ccc(Cc2cccnc2)cc1',\n",
    "        'C(=Cc1ccncn1)c1ccccc1',\n",
    "        'c1ccc(-c2cccnc2)cc1',\n",
    "        'c1ccc(Cc2ccncc2)cc1',\n",
    "        'c1ccc(NC2OCCCO2)cc1',\n",
    "        'C1=CN(Cc2ccccc2)C=CC1',\n",
    "        'C1=C(Cc2ccccc2)CCCC1',\n",
    "        'C1=C(Cc2ccccc2)OCCC1',\n",
    "        'C(=CCCc1ccccc1)C=NCc1ccccc1',\n",
    "        'c1cc(-c2ccncc2)ccn1',\n",
    "        'C1=C(Cc2ccccc2)CCOC1',\n",
    "        'c1ccc(COC2CCCCC2)cc1',\n",
    "        'c1ccc(-c2ncncn2)cc1',\n",
    "        'O=c1ccoc(Cc2ccccc2)c1',\n",
    "        'c1ccc(Cc2ccncn2)cc1',\n",
    "        'c1ccc(-c2ncccn2)cc1',\n",
    "        'C1=C(C2CCCCC2)CCCC1',\n",
    "        'c1ccc(Cc2cccnn2)cc1',\n",
    "        'c1ccc(Cc2nccnn2)cc1',\n",
    "        'c1ccc(Cc2cncnc2)cc1',\n",
    "        'c1ccc(-c2ccccn2)cc1',\n",
    "        'c1ccc(Cc2cncnn2)cc1',\n",
    "        'O=c1cccnn1-c1ccccc1',\n",
    "        'O=c1occcc1Cc1ccccc1',\n",
    "        'C=c1ccc(=Cc2ccccc2)cc1',\n",
    "        'C(=Nc1ccccc1)c1ccccc1',\n",
    "        'O=c1ccoc(CC2=CCCCC2)c1',\n",
    "        'C=C1NC(Cc2ccccc2)=CCS1',\n",
    "        'C1=CC(c2ccccc2)C=CN1',\n",
    "        'O=C1C=CCC=C1Cc1ccccc1',\n",
    "        'c1ccc(CCNCc2ccccc2)cc1',\n",
    "        'C1=CCCC(c2ccccc2)=C1',\n",
    "        'C(=Cc1ccncc1)c1ccccc1',\n",
    "        'C(=NNc1ccccc1)c1ccccc1',\n",
    "        'O=c1cccc(Cc2ccccc2)o1',\n",
    "        'C(=Cc1ncccn1)c1ccccc1',\n",
    "        'c1ccc(CC2=NCCCN2)cc1',\n",
    "        'c1ccc(OCOc2ccccc2)cc1',\n",
    "        'C1=C(Cc2cncnc2)CCCC1',\n",
    "        'c1ccc(-c2cc[o+]cc2)cc1',\n",
    "        'C(=Cc1cccnn1)c1ccccc1',\n",
    "        'C(=NNc1ccccc1)c1ccccn1',\n",
    "        'C1=CN(c2ccccc2)C=CC1',\n",
    "        'O=C1C=C(Nc2ccccc2)CCC1',\n",
    "        'N=c1ccccn1Cn1ccccc1=O',\n",
    "        'C(=Cc1ccccn1)c1ccccc1',\n",
    "        'C(=NNc1ccncn1)c1ccccc1',\n",
    "        'C(=Cc1cnccn1)c1ccccc1',\n",
    "        'C1=CC(NC2CCCOC2)CCC1',\n",
    "        'C=C1C=C(c2ccccc2)OC=N1',\n",
    "        'O=C1C=CC(=C2C=CNC=C2)C=C1',\n",
    "        'O=C(C=Cc1ccccc1)c1cnccn1',\n",
    "        'C=C1C=C(C=Cc2ccccc2)CCC1',\n",
    "        '[N+]=C1C=CC=CC1=Cc1ccccc1',\n",
    "        'c1ccc(CN2CCCCC2)cc1',\n",
    "        'O=C1CCCC(=O)C1=NNc1ccccc1',\n",
    "        'O=C1C=CCC=C1CC1=CCC=CC1=O',\n",
    "        'C(=Cc1cccnc1)Cc1ccccc1',\n",
    "        'C(=NN=Cc1ccccc1)c1ccccc1',\n",
    "        'C(=Cc1cccnn1)C=C1C=COC=C1',\n",
    "        '[O+]=C1C=CC=CC1=Cc1ccccc1',\n",
    "        'C(=Cc1ccncn1)C=C1C=COC=C1',\n",
    "        'C(=Cc1cnccn1)C=C1C=COC=C1',\n",
    "        'C(C=Cc1ccncc1)=Cc1ccccc1',\n",
    "        'c1ccc([N+]#[N+]c2ccccc2)cc1',\n",
    "        'O=C1C=CC(=CC=C2C=CNC=C2)C=C1',\n",
    "        'C(=NCCN=Cc1ccccc1)c1ccccc1',\n",
    "        'C(=Cc1cc[nH+]cc1)c1cc[nH+]cc1',\n",
    "        'C(=Cc1ccccc1)COCCc1ccccc1',\n",
    "        'O=C1NC(=S)NC(=O)C1=Cc1ccccc1',\n",
    "        'O=C1C=CC(=CC2CCC=CC2=O)C=C1',\n",
    "        'C(=CN=Cc1ccccc1)N=Cc1ccccc1',\n",
    "        'O=C(C=Cc1ccccc1)c1cccoc1=O',\n",
    "        'C(=Cc1ccccc1)C=NN=Cc1ccccc1',\n",
    "        'C1=C[CH+]C(=Cc2ccccc2)C=C1',\n",
    "        'O=C1C=CC(=O)C(NCc2ccccc2)=C1',\n",
    "        'N=c1[nH]cncc1CNCCSC(=O)c1ccccc1',\n",
    "        'C(#Cc1ccncc1)c1ccccc1',\n",
    "        'C1=NCSC(Cc2ccccc2)=N1',\n",
    "        'O=C1C=CC(=O)C(c2ccccc2)=C1',\n",
    "        'O=C1C=CC=CC1=CC=C1C=C[NH2+]C=C1',\n",
    "        'C(#Cc1cnccn1)c1ccncc1',\n",
    "        'C(#Cc1cnccn1)c1ccccc1',\n",
    "        'C(=C[NH2+]c1ccccc1)C=Nc1ccccc1',\n",
    "        'C1CCC(OC2CCOCC2)OC1',\n",
    "        'c1ccc(-c2cccnn2)cc1',\n",
    "        'C(#Cc1ncccn1)c1ccccc1',\n",
    "        'C(#Cc1ccncn1)c1ccccc1',\n",
    "        'O=c1cnnc(Cc2ccccc2)o1',\n",
    "        '[N+]=C1C=C=C([CH-]C2=C=CC=C[CH+]2)C=C1',\n",
    "        'O=S(=O)(c1ccccc1)N1CCSCC1',\n",
    "        'c1ncnc(N2CCOCC2)n1',\n",
    "        '[N+]=C1C=CC(=NN=C2C=CC(=[N+])C=C2)C=C1',\n",
    "        'O=C(CCc1ccccc1)Nc1ccccc1',\n",
    "        'O=C1C=NC(=Cc2ccccc2)C(=O)N1',\n",
    "        'C(=NN=Cc1ccccc1)Nc1ccccc1',\n",
    "        'c1ccc(CCC[n+]2ccccc2)cc1',\n",
    "        'C(=Cc1ccccc1)CC1CCCCC1',\n",
    "        '[BH2-]1OC(C=CC=Cc2ccccc2)=CC=[O+]1',\n",
    "        'c1ccc(Cc2ccpcc2)cc1',\n",
    "        'c1ccc(-c2ccncc2)cc1',\n",
    "        'C1=C[N-]C(=C2C=CC=C[N-]2)C=C1',\n",
    "        'C1=CC(=Cc2ccccc2)C=CC1',\n",
    "        'N=C1C=CC(C=C1)=NN=C2C=CC(C=C2)=N',\n",
    "    ],\n",
    "    'Azo': [\n",
    "        'N=N',\n",
    "        'N=[N+]',\n",
    "    ], \n",
    "    'Benz':[\n",
    "        'C1=CC=[NH+]CC=1',\n",
    "        'C1=CCCC=C1',\n",
    "        'C=C1C=CNCC1',\n",
    "        'O=C1C=CC(=O)C=C1',\n",
    "        'O=C1C=CN=CC1',\n",
    "        'O=C1N=CCC=N1',\n",
    "        'S=C1N=CCC=N1',\n",
    "        'c1cc[o+]cc1',\n",
    "        'c1cc[s+]cc1',\n",
    "        'c1ccccc1',\n",
    "        'c1ccncc1',\n",
    "        'c1cnccn1',\n",
    "        'c1cncnc1',\n",
    "        'c1ncncn1',\n",
    "        'c1nncnn1'\n",
    "    ],\n",
    "    }\n",
    "    \n",
    "    print(\"警告：使用了示例骨架定义，请确保获取作者的完整16种骨架定义\")\n",
    "\n",
    "def load_data(input_csv):\n",
    "    \"\"\"加载输入CSV文件\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if 'smiles' not in df.columns:\n",
    "        raise ValueError(\"输入CSV必须包含'smiles'列\")\n",
    "    return df\n",
    "\n",
    "def process_molecules(df):\n",
    "    \"\"\"处理分子并分配骨架标签\"\"\"\n",
    "    # 准备骨架模式\n",
    "    dt = [(k, Chem.MolFromSmiles(m)) for k, v in scaffold.items() for m in v]\n",
    "    scaff_dict = dict([(k, v) for v, k in enumerate(scaffold.keys())])\n",
    "    patterns = pd.DataFrame({\n",
    "        'idx': [scaff_dict[x] for x in list(zip(*dt))[0]],\n",
    "        'mol': list(zip(*dt))[1]\n",
    "    })\n",
    "    \n",
    "    # 分配标签\n",
    "    df['tag'] = -1  # 默认-1表示未分类\n",
    "    for i in tqdm(range(len(df)), desc=\"分配骨架标签\"):\n",
    "        mol = Chem.MolFromSmiles(df.loc[i, 'smiles'])\n",
    "        if mol is None:  # 跳过无效SMILES\n",
    "            continue\n",
    "        for _, patt in patterns.iterrows():\n",
    "            if mol.HasSubstructMatch(patt.mol):\n",
    "                df.loc[i, 'tag'] = patt.idx\n",
    "                break\n",
    "    \n",
    "    # 添加标签名称\n",
    "    scaff_dict_r = dict([(str(v), k) for k, v in scaff_dict.items()])\n",
    "    scaff_dict_r['-1'] = 'None'\n",
    "    df['tag_name'] = [scaff_dict_r[str(t)] for t in df.tag]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_results(df, output_csv):\n",
    "    \"\"\"保存结果\"\"\"\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"结果已保存到 {output_csv}\")\n",
    "\n",
    "def analyze_results(df):\n",
    "    \"\"\"分析结果并打印统计信息\"\"\"\n",
    "    # 统计各骨架类型的数量\n",
    "    tag_counts = df['tag_name'].value_counts()\n",
    "    print(\"\\n骨架类型统计:\")\n",
    "    print(tag_counts)\n",
    "    \n",
    "    # 计算未分类分子的比例\n",
    "    untagged_ratio = len(df[df['tag'] == -1]) / len(df) * 100\n",
    "    print(f\"\\n未分类分子比例: {untagged_ratio:.2f}%\")\n",
    "\n",
    "\n",
    "def main(input_csv, output_csv):\n",
    "    \"\"\"主函数\"\"\"\n",
    "    print(\"开始处理...\")\n",
    "    df = load_data(input_csv)\n",
    "    df = process_molecules(df)\n",
    "    save_results(df, output_csv)\n",
    "    analyze_results(df)\n",
    "    print(\"处理完成!\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = './input/input.csv'  # 替换为你的输入文件路径\n",
    "    output_csv = './input/input.csv'  # 输出文件路径\n",
    "    main(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d12d4-02fd-48fb-a5b9-df32526e20c3",
   "metadata": {},
   "source": [
    "### 1.4 生成染料的mmp指纹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44493936-ce3e-4148-aba3-97250d429f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载子结构: 100%|█████████████████████████████████████████████████████████████████| 136/136 [00:00<00:00, 13772.40it/s]\n",
      "匹配子结构: 100%|██████████████████████████████████████████████████████████████████| 136/136 [00:00<00:00, 2052.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理完成！已添加所有子结构标签列。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取子结构文件\n",
    "substructures_df = pd.read_csv('./data/00_mmp_substructure.csv')\n",
    "substructures = substructures_df['fragment'].tolist()\n",
    "\n",
    "# 读取目标分子文件\n",
    "target_df = pd.read_csv('./input/input.csv')\n",
    "\n",
    "# 检查列是否存在\n",
    "if 'smiles' not in target_df.columns:\n",
    "    raise ValueError(\"目标文件必须包含 'smiles' 列！\")\n",
    "\n",
    "# 将子结构转换为RDKit的Mol对象\n",
    "substructure_mols = []\n",
    "for smarts in tqdm(substructures, desc=\"加载子结构\"):\n",
    "    mol = Chem.MolFromSmarts(smarts)\n",
    "    if mol is None:\n",
    "        print(f\"\\n警告：子结构 '{smarts}' 无效，已跳过。\")\n",
    "    substructure_mols.append(mol)\n",
    "\n",
    "# 定义函数：检查分子是否包含某个子结构\n",
    "def has_substructure(mol, sub_mol):\n",
    "    if mol is None or sub_mol is None:\n",
    "        return 0\n",
    "    return 1 if mol.HasSubstructMatch(sub_mol) else 0\n",
    "\n",
    "# 为每个分子生成所有子结构的标签\n",
    "for i, sub_mol in enumerate(tqdm(substructure_mols, desc=\"匹配子结构\"), start=1):\n",
    "    col_name = f'fragment_{i}'\n",
    "    target_df[col_name] = target_df['smiles'].apply(\n",
    "        lambda s: has_substructure(Chem.MolFromSmiles(s), sub_mol)\n",
    "    )\n",
    "\n",
    "# 保存结果\n",
    "target_df.to_csv('./input/input.csv', index=False)\n",
    "print(\"\\n处理完成！已添加所有子结构标签列。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e284f-c794-4eda-ae08-ab6c31f11d16",
   "metadata": {},
   "source": [
    "### 1.5 生成染料和溶剂的Morgan指纹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80b06c89-17d2-4158-afde-78fb287ef80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指纹生成完成\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# 输入输出路径\n",
    "input_file = './input/input.csv'   # 请替换为你的实际文件路径\n",
    "output_file = './input/target_smiles_morgan.csv'\n",
    "\n",
    "# Morgan指纹参数\n",
    "radius = 2\n",
    "nBits = 1024\n",
    "\n",
    "# 读取 SMILES 数据\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 检查是否包含 \"smiles\" 列\n",
    "if 'smiles' not in df.columns:\n",
    "    raise ValueError(\"输入文件中未找到 'smiles' 列。\")\n",
    "\n",
    "# 初始化输出数据列表\n",
    "fingerprints = []\n",
    "\n",
    "# 遍历每一行生成指纹\n",
    "for idx, smi in enumerate(df['smiles']):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        print(f\"第 {idx} 行 SMILES 无效，跳过。\")\n",
    "        continue\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "    fingerprints.append(list(fp))\n",
    "\n",
    "# 构造 DataFrame\n",
    "fp_columns = [f'{i}' for i in range(nBits)]\n",
    "fp_df = pd.DataFrame(fingerprints, columns=fp_columns)\n",
    "\n",
    "# 保存为 CSV\n",
    "fp_df.to_csv(output_file, index=False)\n",
    "\n",
    "################################## \n",
    "# 输入输出路径\n",
    "input_file = './input/input.csv'   # 请替换为你的实际文件路径\n",
    "output_file = './input/target_sol_morgan.csv'\n",
    "\n",
    "# Morgan指纹参数\n",
    "radius = 2\n",
    "nBits = 1024\n",
    "\n",
    "# 读取 SMILES 数据\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 检查是否包含 \"smiles\" 列\n",
    "if 'solvent' not in df.columns:\n",
    "    raise ValueError(\"输入文件中未找到 'solvent' 列。\")\n",
    "\n",
    "# 初始化输出数据列表\n",
    "fingerprints = []\n",
    "\n",
    "# 遍历每一行生成指纹\n",
    "for idx, smi in enumerate(df['solvent']):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        print(f\"第 {idx} 行 SMILES 无效，跳过。\")\n",
    "        continue\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "    fingerprints.append(list(fp))\n",
    "\n",
    "# 构造 DataFrame\n",
    "fp_columns = [f'{i}' for i in range(nBits)]\n",
    "fp_df = pd.DataFrame(fingerprints, columns=fp_columns)\n",
    "\n",
    "# 保存为 CSV\n",
    "fp_df.to_csv(output_file, index=False)\n",
    "print(f\"指纹生成完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edd7e6-1f1d-4ffa-b3be-185f27294639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe0bf9-8d92-4490-b710-6e48cfed9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义骨架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ec566-7f7f-4317-ad0e-0de344c3b79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c9939-9191-4fef-abda-50d1f7d4062b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6a848-3ec8-41fb-a27c-4e4570e55dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eb5b5-98c9-4d1a-b19a-7cd8cab0b62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21ff52-abe0-4bc4-9512-b1883007ba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981dd78-9412-4a0a-b885-a521eecd4379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654610a-9f7f-4dbd-9824-500316e973a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75a040-b5f7-4a15-93d8-7cd5de182681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa7edc-47f4-40ce-9403-7f7d80251ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1d4b0f7-4a20-446e-bc42-604bd89a0496",
   "metadata": {},
   "source": [
    "# 1. abs最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7609ef-769e-4fa6-835e-0bf48e757ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\dye37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPU\n",
      "n_feats 39 e_feats 10\n",
      "Processing dgl graphs from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\dye37\\lib\\site-packages\\ipykernel_launcher.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Target predictions saved to target_predictions_abs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer\n",
    "from dgllife.data import MoleculeCSVDataset\n",
    "from dgllife.model.gnn import AttentiveFPGNN\n",
    "from dgllife.model.readout import AttentiveFPReadout\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = 'cpu'\n",
    "\n",
    "# 设置全局随机种子\n",
    "seed = 42\n",
    "alpha = 0.1\n",
    "epochs = 3\n",
    "patience = 20\n",
    "n_tasks = 1\n",
    "graph_feat_size = 256 # 固定\n",
    "batch_size = 32       # 固定\n",
    "learning_rate = 1e-3  # 固定\n",
    "\n",
    "\n",
    "dropout = 0.3\n",
    "num_layers = 2\n",
    "num_timesteps = 2 \n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# 使用 AttentiveFP featurizer\n",
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "print(\"n_feats\", n_feats, \"e_feats\", e_feats)\n",
    "\n",
    "node_feat_size = n_feats\n",
    "edge_feat_size = e_feats\n",
    "\n",
    "def compute_lds_weights(targets, h=alpha, sigma=5, sqrt=False, amplify=False):\n",
    "    targets = np.array(targets).reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=sigma).fit(targets)\n",
    "    log_densities = kde.score_samples(targets)\n",
    "    densities = np.exp(log_densities)\n",
    "    weights = 1. / (densities ** h)\n",
    "    if sqrt:\n",
    "        weights = np.sqrt(weights)\n",
    "    if amplify:\n",
    "        median_val = np.median(targets)\n",
    "        weights *= np.where(np.abs(targets - median_val) > 1.5, 2.0, 1.0)\n",
    "    return torch.tensor(weights / np.mean(weights), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def load_data_with_fp(data, fp_data, name, load):\n",
    "    dataset = MoleculeCSVDataset(data,\n",
    "                                 smiles_to_graph=smiles_to_bigraph,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer=bond_featurizer,\n",
    "                                 smiles_column='smiles',\n",
    "                                 cache_file_path=str(name)+'_dataset_abs.bin',\n",
    "                                 task_names=['abs'],\n",
    "                                 load=load, init_mask=True, n_jobs=1)\n",
    "\n",
    "    combined_data = []\n",
    "    for i, data_tuple in enumerate(dataset):\n",
    "        if len(data_tuple) == 3:\n",
    "            smiles, graph, label = data_tuple\n",
    "            mask = None\n",
    "        else:\n",
    "            smiles, graph, label, mask = data_tuple\n",
    "        fp = torch.tensor(fp_data[i], dtype=torch.float32)\n",
    "        combined_data.append((graph, fp, label, mask))\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "#指纹数据加载\n",
    "def load_fingerprints(fp_file):\n",
    "    df = pd.read_csv(fp_file)\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "#数据加载\n",
    "train_data = pd.read_csv('./data/train_abs.csv')\n",
    "valid_data = pd.read_csv('./data/valid_abs.csv')\n",
    "\n",
    "# #数据标准化\n",
    "scaler = StandardScaler()\n",
    "train_data[['abs']] = scaler.fit_transform(train_data[['abs']])\n",
    "valid_data[['abs']] = scaler.transform(valid_data[['abs']])\n",
    "\n",
    "train_fp_solvent = load_fingerprints('./data/train_sol_abs.csv')\n",
    "valid_fp_solvent = load_fingerprints('./data/valid_sol_abs.csv')\n",
    "train_fp_smiles = load_fingerprints('./data/train_smiles_abs.csv')\n",
    "valid_fp_smiles = load_fingerprints('./data/valid_smiles_abs.csv')\n",
    "\n",
    "# === 从 train_data / valid_data 中提取额外特征（列索引 8:152）===\n",
    "train_fp_extra = torch.tensor(train_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "valid_fp_extra = torch.tensor(valid_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "\n",
    "# === 数值部分（8列）归一化 ===\n",
    "scaler_num = MinMaxScaler()\n",
    "\n",
    "# 拆分：前 8 列为数值特征，后面为补充指纹\n",
    "train_num = train_fp_extra[:, :8].numpy()\n",
    "valid_num = valid_fp_extra[:, :8].numpy()\n",
    "\n",
    "train_rest = train_fp_extra[:, 8:]  # tensor 后部分\n",
    "valid_rest = valid_fp_extra[:, 8:]\n",
    "\n",
    "# 拟合并归一化前8列\n",
    "train_num_scaled = scaler_num.fit_transform(train_num)\n",
    "valid_num_scaled = scaler_num.transform(valid_num)\n",
    "\n",
    "# 转换回 tensor 并拼接\n",
    "train_fp_extra = torch.cat([torch.tensor(train_num_scaled, dtype=torch.float32), train_rest], dim=1)\n",
    "valid_fp_extra = torch.cat([torch.tensor(valid_num_scaled, dtype=torch.float32), valid_rest], dim=1)\n",
    "\n",
    "# === 拼接最终特征：solvent + smiles + extra ===\n",
    "train_fp = torch.cat([train_fp_solvent, train_fp_smiles, train_fp_extra], dim=1)\n",
    "valid_fp = torch.cat([valid_fp_solvent, valid_fp_smiles, valid_fp_extra], dim=1)\n",
    "\n",
    "\n",
    "lds_weights = compute_lds_weights(train_data[['abs']].values.flatten())\n",
    "\n",
    "\n",
    "class FingerprintAttentionCNN(nn.Module):\n",
    "    def __init__(self, input_dim, conv_channels=64):\n",
    "        super(FingerprintAttentionCNN, self).__init__()\n",
    "        self.conv_feat = nn.Conv1d(1, conv_channels, kernel_size=3, padding=1)\n",
    "        self.conv_attn = nn.Conv1d(1, conv_channels, kernel_size=3, padding=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B, 1, D]\n",
    "        feat_map = self.conv_feat(x)         # [B, C, D]\n",
    "        attn_map = self.conv_attn(x)         # [B, C, D]\n",
    "        attn_weights = self.softmax(attn_map)\n",
    "        attn_out = torch.sum(feat_map * attn_weights, dim=-1)  # [B, C]\n",
    "        pooled = self.pool(feat_map).squeeze(-1)               # [B, C]\n",
    "        return torch.cat([attn_out, pooled], dim=1)            # [B, 2C]\n",
    "\n",
    "\n",
    "\n",
    "class GraphFingerprintsModel(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size,\n",
    "                solvent_dim, smiles_extra_dim,  # 分开声明两个 fp 输入维度\n",
    "                graph_feat_size=graph_feat_size, num_layers=num_layers, num_timesteps=num_timesteps,\n",
    "                n_tasks=n_tasks, dropout=dropout):\n",
    "        super(GraphFingerprintsModel, self).__init__()\n",
    "\n",
    "        # 图神经网络部分\n",
    "        self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,\n",
    "                                edge_feat_size=edge_feat_size,\n",
    "                                num_layers=num_layers,\n",
    "                                graph_feat_size=graph_feat_size,\n",
    "                                dropout=dropout)\n",
    "        self.readout = AttentiveFPReadout(feat_size=graph_feat_size,\n",
    "                                        num_timesteps=num_timesteps,\n",
    "                                        dropout=dropout)\n",
    "\n",
    "        # 指纹部分一：smiles + extra，使用 CNN-attention 提取\n",
    "        self.fp_extractor = FingerprintAttentionCNN(smiles_extra_dim, conv_channels=graph_feat_size)\n",
    "\n",
    "        # 指纹部分二：solvent，使用全连接提取\n",
    "        self.solvent_extractor = nn.Sequential(\n",
    "            nn.Linear(solvent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, graph_feat_size)\n",
    "        )\n",
    "\n",
    "\n",
    "        total_input_dim = graph_feat_size + graph_feat_size + 2 * graph_feat_size\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_tasks)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats, fingerprints):\n",
    "        if edge_feats is None or 'he' not in g.edata:\n",
    "            num_edges = g.number_of_edges()\n",
    "            edge_feats = torch.zeros((num_edges, edge_feats.size(1)), device=g.device)\n",
    "\n",
    "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
    "        graph_feats = self.readout(g, node_feats)  # [B, G]\n",
    "\n",
    "        # === 分离 fingerprints 三部分 ===\n",
    "        # 假设 fingerprints.shape = [B, S + M + E]（solvent, smiles, extra）\n",
    "        # 你可以根据各自维度切分\n",
    "        B = fingerprints.size(0)\n",
    "        solvent_feat = fingerprints[:, :train_fp_solvent.shape[1]]  # [B, S]\n",
    "        smiles_extra_feat = fingerprints[:, train_fp_solvent.shape[1]:]  # [B, M+E]\n",
    "\n",
    "        # 分别提取特征\n",
    "        solvent_out = self.solvent_extractor(solvent_feat)  # [B, G]\n",
    "        smiles_extra_out = self.fp_extractor(smiles_extra_feat)  # [B, 2G]\n",
    "\n",
    "        # 拼接三部分特征\n",
    "        combined_feats = torch.cat([graph_feats, solvent_out, smiles_extra_out], dim=1)  # [B, 3G]\n",
    "\n",
    "        return self.predict(combined_feats)\n",
    "\n",
    "# 自定义数据集类\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# 数据加载的collate函数\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 5:\n",
    "        graphs, fps, labels, masks, weights = zip(*batch)\n",
    "        weights = torch.stack(weights)\n",
    "    else:\n",
    "        graphs, fps, labels, masks = zip(*batch)\n",
    "        weights = None\n",
    "    graphs = dgl.batch(graphs)\n",
    "    fps = torch.stack(fps)\n",
    "    labels = torch.stack(labels)\n",
    "    masks = torch.stack(masks) if masks[0] is not None else None\n",
    "    return graphs, fps, labels, masks, weights\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for graphs, fps, labels, masks, weights in dataloader:\n",
    "        graphs = graphs.to(device)\n",
    "        fps = fps.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "        if weights is not None:\n",
    "            weights = weights.to(device)\n",
    "        node_feats = graphs.ndata['hv']\n",
    "        edge_feats = graphs.edata['he']\n",
    "        predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "        if masks is not None:\n",
    "            base_loss = (criterion(predictions, labels) * masks).squeeze()\n",
    "        else:\n",
    "            base_loss = criterion(predictions, labels).squeeze()\n",
    "\n",
    "        if weights is not None:\n",
    "            base_loss = base_loss * weights\n",
    "        loss = base_loss.mean()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# === 读取 target 数据 ===\n",
    "target_data = pd.read_csv('./input/input.csv')\n",
    "target_fp_solvent = load_fingerprints('./input/target_sol_morgan.csv')\n",
    "target_fp_smiles = load_fingerprints('./input/target_smiles_morgan.csv')\n",
    "\n",
    "# === 数值特征标准化 ===\n",
    "target_fp_extra = torch.tensor(target_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "target_num = target_fp_extra[:, :8].numpy()\n",
    "target_rest = target_fp_extra[:, 8:]\n",
    "\n",
    "# 加载预训练的 scaler_num\n",
    "target_num_scaled = scaler_num.transform(target_num)\n",
    "target_fp_extra = torch.cat([torch.tensor(target_num_scaled, dtype=torch.float32), target_rest], dim=1)\n",
    "\n",
    "# === 拼接最终指纹 ===\n",
    "target_fp = torch.cat([target_fp_solvent, target_fp_smiles, target_fp_extra], dim=1)\n",
    "\n",
    "# === 标签标准化（只为保持接口一致，其实预测时不需操作标签） ===\n",
    "target_data[['abs']] = scaler.transform(target_data[['abs']])  # 注意：仅为构造 dataset，不影响预测结果\n",
    "\n",
    "# === 构造 dataset 与 dataloader ===\n",
    "target_datasets = load_data_with_fp(target_data, target_fp, 'target', True)\n",
    "target_dataset = MolecularDataset(target_datasets)\n",
    "target_loader = DataLoader(target_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "solvent_dim = target_fp_solvent.shape[1]\n",
    "smiles_extra_dim = target_fp_smiles.shape[1] + target_fp_extra.shape[1]\n",
    "\n",
    "model = GraphFingerprintsModel(\n",
    "    node_feat_size=n_feats,\n",
    "    edge_feat_size=e_feats,\n",
    "    solvent_dim=solvent_dim,\n",
    "    smiles_extra_dim=smiles_extra_dim,\n",
    "    graph_feat_size=graph_feat_size,\n",
    "    num_layers=num_layers,\n",
    "    num_timesteps=num_timesteps,\n",
    "    n_tasks=n_tasks,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# 加载保存的模型参数\n",
    "model.load_state_dict(torch.load('Model_abs.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 预测\n",
    "def predict(model, dataloader):\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 5:\n",
    "                graphs, fps, _, _, _ = batch  # 包含权重\n",
    "            else:\n",
    "                graphs, fps, _, _ = batch     # 不含权重\n",
    "            graphs = graphs.to(device)\n",
    "            fps = fps.to(device)\n",
    "            node_feats = graphs.ndata['hv']\n",
    "            edge_feats = graphs.edata['he']\n",
    "            predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    return np.vstack(all_predictions)\n",
    "\n",
    "\n",
    "# 将预测结果保存到 CSV 文件\n",
    "def save_predictions(predictions, file_name):\n",
    "    df = pd.DataFrame(predictions, columns=['abs'])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# 预测完成后，反向转换标准化的预测结果\n",
    "def reverse_standardization(predictions, scaler):\n",
    "    return scaler.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "# === 模型预测 ===\n",
    "target_predictions = predict(model, target_loader)\n",
    "target_scale_predictions = reverse_standardization(target_predictions, scaler)\n",
    "\n",
    "# === 保存预测结果 ===\n",
    "save_predictions(target_scale_predictions, 'target_predictions_abs.csv')\n",
    "print(\"🎯 Target predictions saved to target_predictions_abs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aedf8eb-579c-43de-b015-faf926a77871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075ae7a-ae03-483c-87da-49493a34751a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a00209d8-9f91-4ba1-8054-7bca4709a9c1",
   "metadata": {},
   "source": [
    "# 2. em最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71dd2f5d-d353-46c1-b096-81945017bf76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPU\n",
      "n_feats 39 e_feats 10\n",
      "Processing dgl graphs from scratch...\n",
      "🎯 Target predictions saved to target_predictions_em.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\dye37\\lib\\site-packages\\ipykernel_launcher.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# nohup python 02_em.py > 02_em.out 2>&1 &\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer\n",
    "from dgllife.data import MoleculeCSVDataset\n",
    "from dgllife.model.gnn import AttentiveFPGNN\n",
    "from dgllife.model.readout import AttentiveFPReadout\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = 'cpu'\n",
    "\n",
    "# 设置全局随机种子\n",
    "seed = 42\n",
    "graph_feat_size = 256\n",
    "alpha = 0\n",
    "num_layers = 3\n",
    "num_timesteps = 1\n",
    "n_tasks = 1\n",
    "dropout = 0.3\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "patience = 20\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# 使用 AttentiveFP featurizer\n",
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "print(\"n_feats\", n_feats, \"e_feats\", e_feats)\n",
    "\n",
    "node_feat_size = n_feats\n",
    "edge_feat_size = e_feats\n",
    "\n",
    "\n",
    "\n",
    "def compute_lds_weights(targets, h=alpha, sigma=5, sqrt=False, amplify=False):\n",
    "    targets = np.array(targets).reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=sigma).fit(targets)\n",
    "    log_densities = kde.score_samples(targets)\n",
    "    densities = np.exp(log_densities)\n",
    "    weights = 1. / (densities ** h)\n",
    "    if sqrt:\n",
    "        weights = np.sqrt(weights)\n",
    "    if amplify:\n",
    "        median_val = np.median(targets)\n",
    "        weights *= np.where(np.abs(targets - median_val) > 1.5, 2.0, 1.0)\n",
    "    return torch.tensor(weights / np.mean(weights), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def load_data_with_fp(data, fp_data, name, load):\n",
    "    dataset = MoleculeCSVDataset(data,\n",
    "                                 smiles_to_graph=smiles_to_bigraph,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer=bond_featurizer,\n",
    "                                 smiles_column='smiles',\n",
    "                                 cache_file_path=str(name)+'_dataset_em.bin',\n",
    "                                 task_names=['em'],\n",
    "                                 load=load, init_mask=True, n_jobs=1)\n",
    "\n",
    "    combined_data = []\n",
    "    for i, data_tuple in enumerate(dataset):\n",
    "        if len(data_tuple) == 3:\n",
    "            smiles, graph, label = data_tuple\n",
    "            mask = None\n",
    "        else:\n",
    "            smiles, graph, label, mask = data_tuple\n",
    "        fp = torch.tensor(fp_data[i], dtype=torch.float32)\n",
    "        combined_data.append((graph, fp, label, mask))\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "#指纹数据加载\n",
    "def load_fingerprints(fp_file):\n",
    "    df = pd.read_csv(fp_file)\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "#数据加载\n",
    "train_data = pd.read_csv('./data/train_em.csv')\n",
    "valid_data = pd.read_csv('./data/valid_em.csv')\n",
    "\n",
    "#数据标准化\n",
    "scaler = StandardScaler()\n",
    "train_data[['em']] = scaler.fit_transform(train_data[['em']])\n",
    "valid_data[['em']] = scaler.transform(valid_data[['em']])\n",
    "\n",
    "train_fp_solvent = load_fingerprints('./data/train_sol_em.csv')\n",
    "valid_fp_solvent = load_fingerprints('./data/valid_sol_em.csv')\n",
    "train_fp_smiles = load_fingerprints('./data/train_smiles_em.csv')\n",
    "valid_fp_smiles = load_fingerprints('./data/valid_smiles_em.csv')\n",
    "\n",
    "# === 从 train_data / valid_data 中提取额外特征（列索引 8:152）===\n",
    "train_fp_extra = torch.tensor(train_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "valid_fp_extra = torch.tensor(valid_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "\n",
    "# === 数值部分（8列）归一化 ===\n",
    "scaler_num = MinMaxScaler()\n",
    "\n",
    "# 拆分：前 8 列为数值特征，后面为补充指纹\n",
    "train_num = train_fp_extra[:, :8].numpy()\n",
    "valid_num = valid_fp_extra[:, :8].numpy()\n",
    "\n",
    "train_rest = train_fp_extra[:, 8:]  # tensor 后部分\n",
    "valid_rest = valid_fp_extra[:, 8:]\n",
    "\n",
    "# 拟合并归一化前8列\n",
    "train_num_scaled = scaler_num.fit_transform(train_num)\n",
    "valid_num_scaled = scaler_num.transform(valid_num)\n",
    "\n",
    "# 转换回 tensor 并拼接\n",
    "train_fp_extra = torch.cat([torch.tensor(train_num_scaled, dtype=torch.float32), train_rest], dim=1)\n",
    "valid_fp_extra = torch.cat([torch.tensor(valid_num_scaled, dtype=torch.float32), valid_rest], dim=1)\n",
    "\n",
    "# === 拼接最终特征：solvent + smiles + extra ===\n",
    "train_fp = torch.cat([train_fp_solvent, train_fp_smiles, train_fp_extra], dim=1)\n",
    "valid_fp = torch.cat([valid_fp_solvent, valid_fp_smiles, valid_fp_extra], dim=1)\n",
    "\n",
    "# train_datasets = load_data_with_fp(train_data, train_fp, 'train', True)\n",
    "# valid_datasets = load_data_with_fp(valid_data, valid_fp, 'valid', True)\n",
    "\n",
    "lds_weights = compute_lds_weights(train_data[['em']].values.flatten())\n",
    "# for i in range(len(train_datasets)):\n",
    "#     g, fp, label, mask = train_datasets[i]\n",
    "#     w = lds_weights[i]\n",
    "#     train_datasets[i] = (g, fp, label, mask, w)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FingerprintAttentionCNN(nn.Module):\n",
    "    def __init__(self, input_dim, conv_channels=64):\n",
    "        super(FingerprintAttentionCNN, self).__init__()\n",
    "        self.conv_feat = nn.Conv1d(1, conv_channels, kernel_size=3, padding=1)\n",
    "        self.conv_attn = nn.Conv1d(1, conv_channels, kernel_size=3, padding=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B, 1, D]\n",
    "        feat_map = self.conv_feat(x)         # [B, C, D]\n",
    "        attn_map = self.conv_attn(x)         # [B, C, D]\n",
    "        attn_weights = self.softmax(attn_map)\n",
    "        attn_out = torch.sum(feat_map * attn_weights, dim=-1)  # [B, C]\n",
    "        pooled = self.pool(feat_map).squeeze(-1)               # [B, C]\n",
    "        return torch.cat([attn_out, pooled], dim=1)            # [B, 2C]\n",
    "\n",
    "\n",
    "class GraphFingerprintsModel(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size,\n",
    "                 solvent_dim, smiles_extra_dim,  # 分开声明两个 fp 输入维度\n",
    "                 graph_feat_size=graph_feat_size, num_layers=num_layers, num_timesteps=num_timesteps,\n",
    "                 n_tasks=n_tasks, dropout=dropout):\n",
    "        super(GraphFingerprintsModel, self).__init__()\n",
    "\n",
    "        # 图神经网络部分\n",
    "        self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=num_layers,\n",
    "                                  graph_feat_size=graph_feat_size,\n",
    "                                  dropout=dropout)\n",
    "        self.readout = AttentiveFPReadout(feat_size=graph_feat_size,\n",
    "                                          num_timesteps=num_timesteps,\n",
    "                                          dropout=dropout)\n",
    "\n",
    "        # 指纹部分一：smiles + extra，使用 CNN-attention 提取\n",
    "        self.fp_extractor = FingerprintAttentionCNN(smiles_extra_dim, conv_channels=graph_feat_size)\n",
    "\n",
    "        # 指纹部分二：solvent，使用全连接提取\n",
    "        self.solvent_extractor = nn.Sequential(\n",
    "            nn.Linear(solvent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, graph_feat_size)\n",
    "        )\n",
    "\n",
    "        total_input_dim = graph_feat_size + graph_feat_size + 2 * graph_feat_size\n",
    "        # 最终拼接后预测（3 * graph_feat_size）\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_tasks)\n",
    "        )\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats, fingerprints):\n",
    "        if edge_feats is None or 'he' not in g.edata:\n",
    "            num_edges = g.number_of_edges()\n",
    "            edge_feats = torch.zeros((num_edges, edge_feats.size(1)), device=g.device)\n",
    "\n",
    "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
    "        graph_feats = self.readout(g, node_feats)  # [B, G]\n",
    "\n",
    "        # === 分离 fingerprints 三部分 ===\n",
    "        # 假设 fingerprints.shape = [B, S + M + E]（solvent, smiles, extra）\n",
    "        # 你可以根据各自维度切分\n",
    "        B = fingerprints.size(0)\n",
    "        solvent_feat = fingerprints[:, :train_fp_solvent.shape[1]]  # [B, S]\n",
    "        smiles_extra_feat = fingerprints[:, train_fp_solvent.shape[1]:]  # [B, M+E]\n",
    "\n",
    "        # 分别提取特征\n",
    "        solvent_out = self.solvent_extractor(solvent_feat)  # [B, G]\n",
    "        smiles_extra_out = self.fp_extractor(smiles_extra_feat)  # [B, 2G]\n",
    "\n",
    "        # 拼接三部分特征\n",
    "        combined_feats = torch.cat([graph_feats, solvent_out, smiles_extra_out], dim=1)  # [B, 3G]\n",
    "\n",
    "        return self.predict(combined_feats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 自定义数据集类\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# 数据加载的collate函数\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 5:\n",
    "        graphs, fps, labels, masks, weights = zip(*batch)\n",
    "        weights = torch.stack(weights)\n",
    "    else:\n",
    "        graphs, fps, labels, masks = zip(*batch)\n",
    "        weights = None\n",
    "    graphs = dgl.batch(graphs)\n",
    "    fps = torch.stack(fps)\n",
    "    labels = torch.stack(labels)\n",
    "    masks = torch.stack(masks) if masks[0] is not None else None\n",
    "    return graphs, fps, labels, masks, weights\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for graphs, fps, labels, masks, weights in dataloader:\n",
    "        graphs = graphs.to(device)\n",
    "        fps = fps.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "        if weights is not None:\n",
    "            weights = weights.to(device)\n",
    "        node_feats = graphs.ndata['hv']\n",
    "        edge_feats = graphs.edata['he']\n",
    "        predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "        if masks is not None:\n",
    "            base_loss = (criterion(predictions, labels) * masks).squeeze()\n",
    "        else:\n",
    "            base_loss = criterion(predictions, labels).squeeze()\n",
    "\n",
    "        if weights is not None:\n",
    "            base_loss = base_loss * weights\n",
    "        loss = base_loss.mean()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# === 读取 target 数据 ===\n",
    "target_data = pd.read_csv('./input/input.csv')\n",
    "target_fp_solvent = load_fingerprints('./input/target_sol_morgan.csv')\n",
    "target_fp_smiles = load_fingerprints('./input/target_smiles_morgan.csv')\n",
    "\n",
    "# === 数值特征标准化 ===\n",
    "target_fp_extra = torch.tensor(target_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "target_num = target_fp_extra[:, :8].numpy()\n",
    "target_rest = target_fp_extra[:, 8:]\n",
    "\n",
    "# 加载预训练的 scaler_num\n",
    "target_num_scaled = scaler_num.transform(target_num)\n",
    "target_fp_extra = torch.cat([torch.tensor(target_num_scaled, dtype=torch.float32), target_rest], dim=1)\n",
    "\n",
    "# === 拼接最终指纹 ===\n",
    "target_fp = torch.cat([target_fp_solvent, target_fp_smiles, target_fp_extra], dim=1)\n",
    "\n",
    "# === 标签标准化（只为保持接口一致，其实预测时不需操作标签） ===\n",
    "target_data[['em']] = scaler.transform(target_data[['em']])  # 注意：仅为构造 dataset，不影响预测结果\n",
    "\n",
    "# === 构造 dataset 与 dataloader ===\n",
    "target_datasets = load_data_with_fp(target_data, target_fp, 'target', True)\n",
    "target_dataset = MolecularDataset(target_datasets)\n",
    "target_loader = DataLoader(target_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "solvent_dim = target_fp_solvent.shape[1]\n",
    "smiles_extra_dim = target_fp_smiles.shape[1] + target_fp_extra.shape[1]\n",
    "\n",
    "model = GraphFingerprintsModel(\n",
    "    node_feat_size=n_feats,\n",
    "    edge_feat_size=e_feats,\n",
    "    solvent_dim=solvent_dim,\n",
    "    smiles_extra_dim=smiles_extra_dim,\n",
    "    graph_feat_size=graph_feat_size,\n",
    "    num_layers=num_layers,\n",
    "    num_timesteps=num_timesteps,\n",
    "    n_tasks=n_tasks,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# 加载保存的模型参数\n",
    "model.load_state_dict(torch.load('Model_em.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 预测\n",
    "def predict(model, dataloader):\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 5:\n",
    "                graphs, fps, _, _, _ = batch  # 包含权重\n",
    "            else:\n",
    "                graphs, fps, _, _ = batch     # 不含权重\n",
    "            graphs = graphs.to(device)\n",
    "            fps = fps.to(device)\n",
    "            node_feats = graphs.ndata['hv']\n",
    "            edge_feats = graphs.edata['he']\n",
    "            predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    return np.vstack(all_predictions)\n",
    "\n",
    "\n",
    "# 将预测结果保存到 CSV 文件\n",
    "def save_predictions(predictions, file_name):\n",
    "    df = pd.DataFrame(predictions, columns=['em'])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# 预测完成后，反向转换标准化的预测结果\n",
    "def reverse_standardization(predictions, scaler):\n",
    "    return scaler.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "# === 模型预测 ===\n",
    "target_predictions = predict(model, target_loader)\n",
    "target_scale_predictions = reverse_standardization(target_predictions, scaler)\n",
    "\n",
    "# === 保存预测结果 ===\n",
    "save_predictions(target_scale_predictions, 'target_predictions_em.csv')\n",
    "print(\"🎯 Target predictions saved to target_predictions_em.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a220995-d03c-43fa-a602-38de704ede56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f261-17a2-4974-be8d-018fd67b5ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f453b58-d6ec-4331-81a0-a98f7f5dfa25",
   "metadata": {},
   "source": [
    "# 3. plqy最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6921a472-3531-4f76-9bb9-05880fa88dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPU\n",
      "Processing dgl graphs from scratch...\n",
      "🎯 Target predictions saved to target_predictions_plqy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\dye37\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# nohup python best_plqy_权重计算.py > plqy.out 2>&1 &\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer\n",
    "from dgllife.data import MoleculeCSVDataset\n",
    "from dgllife.model.gnn import AttentiveFPGNN\n",
    "from dgllife.model.readout import AttentiveFPReadout\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "seed = 42\n",
    "graph_feat_size = 256\n",
    "n_tasks = 1\n",
    "dropout = 0.4\n",
    "alpha = 0.2\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "patience = 20\n",
    "num_layers = 2\n",
    "num_timesteps = 3\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_lds_weights(targets, h=alpha, sigma=5, sqrt=False, amplify=False):\n",
    "    targets = np.array(targets).reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=sigma).fit(targets)\n",
    "    log_densities = kde.score_samples(targets)\n",
    "    densities = np.exp(log_densities)\n",
    "    weights = 1. / (densities ** h)\n",
    "    if sqrt:\n",
    "        weights = np.sqrt(weights)\n",
    "    if amplify:\n",
    "        median_val = np.median(targets)\n",
    "        weights *= np.where(np.abs(targets - median_val) > 1.5, 2.0, 1.0)\n",
    "    return torch.tensor(weights / np.mean(weights), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "\n",
    "\n",
    "def load_data_with_fp(data, fp_data, name, load):\n",
    "    dataset = MoleculeCSVDataset(data,\n",
    "                                 smiles_to_graph=smiles_to_bigraph,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer=bond_featurizer,\n",
    "                                 smiles_column='smiles',\n",
    "                                 cache_file_path=str(name)+'_dataset_plqy.bin',\n",
    "                                 task_names=['plqy'],\n",
    "                                 load=load, init_mask=True, n_jobs=1)\n",
    "\n",
    "    combined_data = []\n",
    "    for i, data_tuple in enumerate(dataset):\n",
    "        if len(data_tuple) == 3:\n",
    "            smiles, graph, label = data_tuple\n",
    "            mask = None\n",
    "        else:\n",
    "            smiles, graph, label, mask = data_tuple\n",
    "        fp = torch.tensor(fp_data[i], dtype=torch.float32)\n",
    "        combined_data.append((graph, fp, label, mask))\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def load_fingerprints(fp_file):\n",
    "    df = pd.read_csv(fp_file)\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "train_data = pd.read_csv('./data/train_plqy.csv')\n",
    "valid_data = pd.read_csv('./data/valid_plqy.csv')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[['plqy']] = scaler.fit_transform(train_data[['plqy']])\n",
    "valid_data[['plqy']] = scaler.transform(valid_data[['plqy']])\n",
    "\n",
    "train_fp_solvent = load_fingerprints('./data/train_sol_plqy.csv')\n",
    "valid_fp_solvent = load_fingerprints('./data/valid_sol_plqy.csv')\n",
    "train_fp_smiles = load_fingerprints('./data/train_smiles_plqy.csv')\n",
    "valid_fp_smiles = load_fingerprints('./data/valid_smiles_plqy.csv')\n",
    "\n",
    "train_fp_extra = torch.tensor(train_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "valid_fp_extra = torch.tensor(valid_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "scaler_num = MinMaxScaler()\n",
    "train_num = train_fp_extra[:, :8].numpy()\n",
    "valid_num = valid_fp_extra[:, :8].numpy()\n",
    "train_rest = train_fp_extra[:, 8:]\n",
    "valid_rest = valid_fp_extra[:, 8:]\n",
    "train_num_scaled = scaler_num.fit_transform(train_num)\n",
    "valid_num_scaled = scaler_num.transform(valid_num)\n",
    "train_fp_extra = torch.cat([torch.tensor(train_num_scaled, dtype=torch.float32), train_rest], dim=1)\n",
    "valid_fp_extra = torch.cat([torch.tensor(valid_num_scaled, dtype=torch.float32), valid_rest], dim=1)\n",
    "train_fp = torch.cat([train_fp_solvent, train_fp_smiles, train_fp_extra], dim=1)\n",
    "valid_fp = torch.cat([valid_fp_solvent, valid_fp_smiles, valid_fp_extra], dim=1)\n",
    "\n",
    "\n",
    "lds_weights = compute_lds_weights(train_data[['plqy']].values.flatten())\n",
    "\n",
    "class GraphFingerprintsModel(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size, fp_size, \n",
    "                 graph_feat_size=graph_feat_size, num_layers=num_layers, num_timesteps=num_timesteps, \n",
    "                 n_tasks=n_tasks, dropout=dropout):\n",
    "        super(GraphFingerprintsModel, self).__init__()\n",
    "        self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=num_layers,\n",
    "                                  graph_feat_size=graph_feat_size,\n",
    "                                  dropout=dropout)\n",
    "        self.readout = AttentiveFPReadout(feat_size=graph_feat_size,\n",
    "                                          num_timesteps=num_timesteps,\n",
    "                                          dropout=dropout)\n",
    "        self.fp_fc = nn.Sequential(\n",
    "            nn.Linear(fp_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, graph_feat_size)\n",
    "        )\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(graph_feat_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_tasks)\n",
    "        )\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats, fingerprints):\n",
    "        if edge_feats is None or 'he' not in g.edata.keys():\n",
    "            num_edges = g.number_of_edges()\n",
    "            edge_feats = torch.zeros((num_edges, e_feats)).to(g.device)\n",
    "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
    "        graph_feats = self.readout(g, node_feats)\n",
    "        fp_feats = self.fp_fc(fingerprints)\n",
    "        combined_feats = torch.cat([graph_feats, fp_feats], dim=1)\n",
    "        return self.predict(combined_feats)\n",
    "\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 5:\n",
    "        graphs, fps, labels, masks, weights = zip(*batch)\n",
    "        weights = torch.stack(weights)\n",
    "    else:\n",
    "        graphs, fps, labels, masks = zip(*batch)\n",
    "        weights = None\n",
    "    graphs = dgl.batch(graphs)\n",
    "    fps = torch.stack(fps)\n",
    "    labels = torch.stack(labels)\n",
    "    masks = torch.stack(masks) if masks[0] is not None else None\n",
    "    return graphs, fps, labels, masks, weights\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for graphs, fps, labels, masks, weights in dataloader:\n",
    "        graphs = graphs.to(device)\n",
    "        fps = fps.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "        if weights is not None:\n",
    "            weights = weights.to(device)\n",
    "\n",
    "        node_feats = graphs.ndata['hv']\n",
    "        edge_feats = graphs.edata['he']\n",
    "        predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "\n",
    "        if masks is not None:\n",
    "            base_loss = (criterion(predictions, labels) * masks).squeeze()\n",
    "        else:\n",
    "            base_loss = criterion(predictions, labels).squeeze()\n",
    "\n",
    "        if weights is not None:\n",
    "            base_loss = base_loss * weights\n",
    "\n",
    "        loss = base_loss.mean()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_dataset = MolecularDataset(train_datasets)\n",
    "# valid_dataset = MolecularDataset(valid_datasets)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# === 读取 target 数据 ===\n",
    "target_data = pd.read_csv('./input/input.csv')\n",
    "target_fp_solvent = load_fingerprints('./input/target_sol_morgan.csv')\n",
    "target_fp_smiles = load_fingerprints('./input/target_smiles_morgan.csv')\n",
    "\n",
    "# === 数值特征标准化 ===\n",
    "target_fp_extra = torch.tensor(target_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "target_num = target_fp_extra[:, :8].numpy()\n",
    "target_rest = target_fp_extra[:, 8:]\n",
    "\n",
    "# 加载预训练的 scaler_num\n",
    "target_num_scaled = scaler_num.transform(target_num)\n",
    "target_fp_extra = torch.cat([torch.tensor(target_num_scaled, dtype=torch.float32), target_rest], dim=1)\n",
    "\n",
    "# === 拼接最终指纹 ===\n",
    "target_fp = torch.cat([target_fp_solvent, target_fp_smiles, target_fp_extra], dim=1)\n",
    "\n",
    "# === 标签标准化（只为保持接口一致，其实预测时不需操作标签） ===\n",
    "target_data[['plqy']] = scaler.transform(target_data[['plqy']])  # 注意：仅为构造 dataset，不影响预测结果\n",
    "\n",
    "# === 构造 dataset 与 dataloader ===\n",
    "target_datasets = load_data_with_fp(target_data, target_fp, 'target', True)\n",
    "target_dataset = MolecularDataset(target_datasets)\n",
    "target_loader = DataLoader(target_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fp_size = target_fp.shape[1]\n",
    "# 初始化模型\n",
    "model = GraphFingerprintsModel(node_feat_size=n_feats,\n",
    "                               edge_feat_size=e_feats,\n",
    "                               graph_feat_size=graph_feat_size,\n",
    "                               num_layers=num_layers,\n",
    "                               num_timesteps=num_timesteps,\n",
    "                               fp_size=fp_size,\n",
    "                               n_tasks=n_tasks,\n",
    "                               dropout=dropout).to(device)\n",
    "\n",
    "# 加载保存的模型参数\n",
    "model.load_state_dict(torch.load('Model_plqy.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 预测\n",
    "def predict(model, dataloader):\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 5:\n",
    "                graphs, fps, _, _, _ = batch  # 包含权重\n",
    "            else:\n",
    "                graphs, fps, _, _ = batch     # 不含权重\n",
    "            graphs = graphs.to(device)\n",
    "            fps = fps.to(device)\n",
    "            node_feats = graphs.ndata['hv']\n",
    "            edge_feats = graphs.edata['he']\n",
    "            predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    return np.vstack(all_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# 将预测结果保存到 CSV 文件\n",
    "def save_predictions(predictions, file_name):\n",
    "    df = pd.DataFrame(predictions, columns=['plqy'])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# 预测完成后，反向转换标准化的预测结果\n",
    "def reverse_standardization(predictions, scaler):\n",
    "    return scaler.inverse_transform(predictions)\n",
    "\n",
    "# === 模型预测 ===\n",
    "target_predictions = predict(model, target_loader)\n",
    "target_scale_predictions = reverse_standardization(target_predictions, scaler)\n",
    "\n",
    "# === 保存预测结果 ===\n",
    "save_predictions(target_scale_predictions, 'target_predictions_plqy.csv')\n",
    "print(\"🎯 Target predictions saved to target_predictions_plqy.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a467181-da0a-4b3e-9fd6-3d9f550cfa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679efb8d-c702-4e6c-a839-31865d6da983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e744241-3805-432c-9487-e6eaeb711cca",
   "metadata": {},
   "source": [
    "# 4. k 最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2379ebb1-6b6d-4a73-bc49-b7616b2aa41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPU\n",
      "Processing dgl graphs from scratch...\n",
      "🎯 Target predictions saved to target_predictions_k.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\dye37\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- abs\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- k\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "E:\\Anaconda\\envs\\dye37\\lib\\site-packages\\ipykernel_launcher.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# nohup python best_k_权重计算.py > k.out 2>&1 &\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer\n",
    "from dgllife.data import MoleculeCSVDataset\n",
    "from dgllife.model.gnn import AttentiveFPGNN\n",
    "from dgllife.model.readout import AttentiveFPReadout\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = 'cpu'\n",
    "\n",
    "seed = 42\n",
    "graph_feat_size = 256\n",
    "n_tasks = 1\n",
    "dropout = 0.3\n",
    "alpha = 0.6\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "patience = 20\n",
    "num_layers = 3\n",
    "num_timesteps = 1\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_lds_weights(targets, h=alpha, sigma=5, sqrt=False, amplify=False):\n",
    "    targets = np.array(targets).reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=sigma).fit(targets)\n",
    "    log_densities = kde.score_samples(targets)\n",
    "    densities = np.exp(log_densities)\n",
    "    weights = 1. / (densities ** h)\n",
    "    if sqrt:\n",
    "        weights = np.sqrt(weights)\n",
    "    if amplify:\n",
    "        median_val = np.median(targets)\n",
    "        weights *= np.where(np.abs(targets - median_val) > 1.5, 2.0, 1.0)\n",
    "    return torch.tensor(weights / np.mean(weights), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "\n",
    "\n",
    "def load_data_with_fp(data, fp_data, name, load):\n",
    "    dataset = MoleculeCSVDataset(data,\n",
    "                                 smiles_to_graph=smiles_to_bigraph,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer=bond_featurizer,\n",
    "                                 smiles_column='smiles',\n",
    "                                 cache_file_path=str(name)+'_dataset_k.bin',\n",
    "                                 task_names=['k'],\n",
    "                                 load=load, init_mask=True, n_jobs=1)\n",
    "\n",
    "    combined_data = []\n",
    "    for i, data_tuple in enumerate(dataset):\n",
    "        if len(data_tuple) == 3:\n",
    "            smiles, graph, label = data_tuple\n",
    "            mask = None\n",
    "        else:\n",
    "            smiles, graph, label, mask = data_tuple\n",
    "        fp = torch.tensor(fp_data[i], dtype=torch.float32)\n",
    "        combined_data.append((graph, fp, label, mask))\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def load_fingerprints(fp_file):\n",
    "    df = pd.read_csv(fp_file)\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "train_data = pd.read_csv('./data/train_k.csv')\n",
    "valid_data = pd.read_csv('./data/valid_k.csv')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[['k']] = scaler.fit_transform(train_data[['k']])\n",
    "valid_data[['k']] = scaler.transform(valid_data[['k']])\n",
    "\n",
    "train_fp_solvent = load_fingerprints('./data/train_sol_k.csv')\n",
    "valid_fp_solvent = load_fingerprints('./data/valid_sol_k.csv')\n",
    "train_fp_smiles = load_fingerprints('./data/train_smiles_k.csv')\n",
    "valid_fp_smiles = load_fingerprints('./data/valid_smiles_k.csv')\n",
    "\n",
    "train_fp_extra = torch.tensor(train_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "valid_fp_extra = torch.tensor(valid_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "scaler_num = MinMaxScaler()\n",
    "train_num = train_fp_extra[:, :8].numpy()\n",
    "valid_num = valid_fp_extra[:, :8].numpy()\n",
    "train_rest = train_fp_extra[:, 8:]\n",
    "valid_rest = valid_fp_extra[:, 8:]\n",
    "train_num_scaled = scaler_num.fit_transform(train_num)\n",
    "valid_num_scaled = scaler_num.transform(valid_num)\n",
    "train_fp_extra = torch.cat([torch.tensor(train_num_scaled, dtype=torch.float32), train_rest], dim=1)\n",
    "valid_fp_extra = torch.cat([torch.tensor(valid_num_scaled, dtype=torch.float32), valid_rest], dim=1)\n",
    "train_fp = torch.cat([train_fp_solvent, train_fp_smiles, train_fp_extra], dim=1)\n",
    "valid_fp = torch.cat([valid_fp_solvent, valid_fp_smiles, valid_fp_extra], dim=1)\n",
    "\n",
    "\n",
    "lds_weights = compute_lds_weights(train_data[['k']].values.flatten())\n",
    "\n",
    "\n",
    "\n",
    "class GraphFingerprintsModel(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size, fp_size, \n",
    "                 graph_feat_size=graph_feat_size, num_layers=num_layers, num_timesteps=num_timesteps, \n",
    "                 n_tasks=n_tasks, dropout=dropout):\n",
    "        super(GraphFingerprintsModel, self).__init__()\n",
    "        self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=num_layers,\n",
    "                                  graph_feat_size=graph_feat_size,\n",
    "                                  dropout=dropout)\n",
    "        self.readout = AttentiveFPReadout(feat_size=graph_feat_size,\n",
    "                                          num_timesteps=num_timesteps,\n",
    "                                          dropout=dropout)\n",
    "        self.fp_fc = nn.Sequential(\n",
    "            nn.Linear(fp_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, graph_feat_size)\n",
    "        )\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(graph_feat_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_tasks)\n",
    "        )\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats, fingerprints):\n",
    "        if edge_feats is None or 'he' not in g.edata.keys():\n",
    "            num_edges = g.number_of_edges()\n",
    "            edge_feats = torch.zeros((num_edges, e_feats)).to(g.device)\n",
    "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
    "        graph_feats = self.readout(g, node_feats)\n",
    "        fp_feats = self.fp_fc(fingerprints)\n",
    "        combined_feats = torch.cat([graph_feats, fp_feats], dim=1)\n",
    "        return self.predict(combined_feats)\n",
    "\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 5:\n",
    "        graphs, fps, labels, masks, weights = zip(*batch)\n",
    "        weights = torch.stack(weights)\n",
    "    else:\n",
    "        graphs, fps, labels, masks = zip(*batch)\n",
    "        weights = None\n",
    "    graphs = dgl.batch(graphs)\n",
    "    fps = torch.stack(fps)\n",
    "    labels = torch.stack(labels)\n",
    "    masks = torch.stack(masks) if masks[0] is not None else None\n",
    "    return graphs, fps, labels, masks, weights\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for graphs, fps, labels, masks, weights in dataloader:\n",
    "        graphs = graphs.to(device)\n",
    "        fps = fps.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "        if weights is not None:\n",
    "            weights = weights.to(device)\n",
    "        node_feats = graphs.ndata['hv']\n",
    "        edge_feats = graphs.edata['he']\n",
    "        predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "        if masks is not None:\n",
    "            base_loss = (criterion(predictions, labels) * masks).squeeze()\n",
    "        else:\n",
    "            base_loss = criterion(predictions, labels).squeeze()\n",
    "\n",
    "        if weights is not None:\n",
    "            base_loss = base_loss * weights\n",
    "        loss = base_loss.mean()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# === 读取 target 数据 ===\n",
    "target_data = pd.read_csv('./input/input.csv')\n",
    "target_fp_solvent = load_fingerprints('./input/target_sol_morgan.csv')\n",
    "target_fp_smiles = load_fingerprints('./input/target_smiles_morgan.csv')\n",
    "\n",
    "# === 数值特征标准化 ===\n",
    "target_fp_extra = torch.tensor(target_data.iloc[:, 8:152].values, dtype=torch.float32)\n",
    "target_num = target_fp_extra[:, :8].numpy()\n",
    "target_rest = target_fp_extra[:, 8:]\n",
    "\n",
    "# 加载预训练的 scaler_num\n",
    "target_num_scaled = scaler_num.transform(target_num)\n",
    "target_fp_extra = torch.cat([torch.tensor(target_num_scaled, dtype=torch.float32), target_rest], dim=1)\n",
    "\n",
    "# === 拼接最终指纹 ===\n",
    "target_fp = torch.cat([target_fp_solvent, target_fp_smiles, target_fp_extra], dim=1)\n",
    "\n",
    "# === 标签标准化（只为保持接口一致，其实预测时不需操作标签） ===\n",
    "target_data[['abs']] = scaler.transform(target_data[['abs']])  # 注意：仅为构造 dataset，不影响预测结果\n",
    "\n",
    "# === 构造 dataset 与 dataloader ===\n",
    "target_datasets = load_data_with_fp(target_data, target_fp, 'target', True)\n",
    "target_dataset = MolecularDataset(target_datasets)\n",
    "target_loader = DataLoader(target_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "fp_size = target_fp.shape[1]\n",
    "# 初始化模型\n",
    "model = GraphFingerprintsModel(node_feat_size=n_feats,\n",
    "                               edge_feat_size=e_feats,\n",
    "                               graph_feat_size=graph_feat_size,\n",
    "                               num_layers=num_layers,\n",
    "                               num_timesteps=num_timesteps,\n",
    "                               fp_size=fp_size,\n",
    "                               n_tasks=n_tasks,\n",
    "                               dropout=dropout).to(device)\n",
    "\n",
    "# 加载保存的模型参数\n",
    "model.load_state_dict(torch.load('Model_k.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 预测\n",
    "def predict(model, dataloader):\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 5:\n",
    "                graphs, fps, _, _, _ = batch  # 包含权重\n",
    "            else:\n",
    "                graphs, fps, _, _ = batch     # 不含权重\n",
    "            graphs = graphs.to(device)\n",
    "            fps = fps.to(device)\n",
    "            node_feats = graphs.ndata['hv']\n",
    "            edge_feats = graphs.edata['he']\n",
    "            predictions = model(graphs, node_feats, edge_feats, fps)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    return np.vstack(all_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# 将预测结果保存到 CSV 文件\n",
    "def save_predictions(predictions, file_name):\n",
    "    df = pd.DataFrame(predictions, columns=['k'])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# 预测完成后，反向转换标准化的预测结果\n",
    "def reverse_standardization(predictions, scaler):\n",
    "    return scaler.inverse_transform(predictions)\n",
    "\n",
    "# === 模型预测 ===\n",
    "target_predictions = predict(model, target_loader)\n",
    "target_scale_predictions = reverse_standardization(target_predictions, scaler)\n",
    "\n",
    "# === 保存预测结果 ===\n",
    "save_predictions(target_scale_predictions, 'target_predictions_k.csv')\n",
    "print(\"🎯 Target predictions saved to target_predictions_k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a9aea-a6d3-450a-9599-2f730c917df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dye37",
   "language": "python",
   "name": "dye37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
